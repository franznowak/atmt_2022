{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSaIchQYgSs_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a39c90e-8060-49f9-82b6-0daf45fd48fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "%cd /content/drive/MyDrive/MT/atmt_2022"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POHzmCkgRXU5",
        "outputId": "5b4f95e1-2636-4130-d2de-1dc6228af2f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MT/atmt_2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "hNKfQc_bLhpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy data folder to somewhere else for testing\n",
        "! rm -rf /content/drive/MyDrive/MT/data\n",
        "! cp -r /content/drive/MyDrive/MT/atmt_2022/data /content/drive/MyDrive/MT"
      ],
      "metadata": {
        "id": "Poza-xtjovr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vars to run the preprocess.py script\n",
        "# we start from the already preprocessed data in $preprocessed\n",
        "# we apply the BPE to all the files and save all of them to $BPE\n",
        "lang = 'fr'\n",
        "moses_scripts = '/content/drive/MyDrive/MT/atmt_2022/scripts/../moses_scripts'\n",
        "preprocessed = '/content/drive/MyDrive/MT/data/en-fr/preprocessed'\n",
        "raw_data = '/content/drive/MyDrive/MT/data/en-fr/raw'\n",
        "split = 'train'\n",
        "base = '/content/drive/MyDrive/MT/atmt_2022'\n",
        "src_lang = 'fr'\n",
        "tgt_lang = 'en'\n",
        "prepared = '/content/drive/MyDrive/MT/data/en-fr/prepared'\n",
        "BPE = '/content/drive/MyDrive/MT/data/en-fr/BPE'\n",
        "# $prepared need to be empty\n",
        "\"\"\"\n",
        "! rm -rf $prepared\n",
        "! mkdir $prepared\n",
        "! mkdir $BPE\n",
        "\"\"\"\n",
        "# we have empty BPE and prepared now"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "azzhVspmstNA",
        "outputId": "6496dbd7-2fdb-42b7-bed9-f379db8ff5cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n! rm -rf $prepared\\n! mkdir $prepared\\n! mkdir $BPE\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Evaluate Model"
      ],
      "metadata": {
        "id": "af2wh_EMLxK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -rf /content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_1"
      ],
      "metadata": {
        "id": "6CYl51QxtjUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py \\\n",
        "--data /content/drive/MyDrive/MT/data/en-fr/prepared \\\n",
        "--source-lang $src_lang \\\n",
        "--target-lang $tgt_lang \\\n",
        "--save-dir assignments/03/BPE_1/checkpoints \\\n",
        "--cuda \\\n",
        "--batch-size 256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ADEWWUgRTqP",
        "outputId": "c6ae1027-fb96-48c7-c97f-aaa8d864350a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data /content/drive/MyDrive/MT/data/en-fr/prepared --source-lang fr --target-lang en --save-dir assignments/03/BPE_1/checkpoints --cuda --batch-size 256\n",
            "INFO: Arguments: {'cuda': True, 'data': '/content/drive/MyDrive/MT/data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 256, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/BPE_1/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 7.893 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.696 | clip 0.5\n",
            "INFO: Epoch 000: valid_loss 6.68 | num_tokens 9.99 | batch_size 500 | valid_perplexity 794\n",
            "INFO: Epoch 001: loss 6.08 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 12.63 | clip 1\n",
            "INFO: Epoch 001: valid_loss 5.76 | num_tokens 9.99 | batch_size 500 | valid_perplexity 318\n",
            "INFO: Epoch 002: loss 5.619 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 8.124 | clip 0.7\n",
            "INFO: Epoch 002: valid_loss 5.61 | num_tokens 9.99 | batch_size 500 | valid_perplexity 273\n",
            "INFO: Epoch 003: loss 5.447 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 8.984 | clip 1\n",
            "INFO: Epoch 003: valid_loss 5.51 | num_tokens 9.99 | batch_size 500 | valid_perplexity 248\n",
            "INFO: Epoch 004: loss 5.333 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 9.182 | clip 0.8\n",
            "INFO: Epoch 004: valid_loss 5.4 | num_tokens 9.99 | batch_size 500 | valid_perplexity 221\n",
            "INFO: Epoch 005: loss 5.246 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 9.184 | clip 0.75\n",
            "INFO: Epoch 005: valid_loss 5.35 | num_tokens 9.99 | batch_size 500 | valid_perplexity 210\n",
            "INFO: Epoch 006: loss 5.206 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 10.08 | clip 0.825\n",
            "INFO: Epoch 006: valid_loss 5.3 | num_tokens 9.99 | batch_size 500 | valid_perplexity 200\n",
            "INFO: Epoch 007: loss 5.161 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 10.68 | clip 0.825\n",
            "INFO: Epoch 007: valid_loss 5.25 | num_tokens 9.99 | batch_size 500 | valid_perplexity 191\n",
            "INFO: Epoch 008: loss 5.103 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 9.966 | clip 0.75\n",
            "INFO: Epoch 008: valid_loss 5.23 | num_tokens 9.99 | batch_size 500 | valid_perplexity 186\n",
            "INFO: Epoch 009: loss 5.075 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 10.25 | clip 0.775\n",
            "INFO: Epoch 009: valid_loss 5.2 | num_tokens 9.99 | batch_size 500 | valid_perplexity 181\n",
            "INFO: Epoch 010: loss 5.037 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 10.12 | clip 0.75\n",
            "INFO: Epoch 010: valid_loss 5.17 | num_tokens 9.99 | batch_size 500 | valid_perplexity 176\n",
            "INFO: Epoch 011: loss 5.005 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 9.414 | clip 0.8\n",
            "INFO: Epoch 011: valid_loss 5.14 | num_tokens 9.99 | batch_size 500 | valid_perplexity 170\n",
            "INFO: Epoch 012: loss 4.962 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 9.439 | clip 0.8\n",
            "INFO: Epoch 012: valid_loss 5.11 | num_tokens 9.99 | batch_size 500 | valid_perplexity 165\n",
            "INFO: Epoch 013: loss 4.918 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 9.442 | clip 0.8\n",
            "INFO: Epoch 013: valid_loss 5.05 | num_tokens 9.99 | batch_size 500 | valid_perplexity 157\n",
            "INFO: Epoch 014: loss 4.875 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 9.247 | clip 0.8\n",
            "INFO: Epoch 014: valid_loss 5.03 | num_tokens 9.99 | batch_size 500 | valid_perplexity 154\n",
            "INFO: Epoch 015: loss 4.844 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 9.109 | clip 0.8\n",
            "INFO: Epoch 015: valid_loss 5 | num_tokens 9.99 | batch_size 500 | valid_perplexity 148\n",
            "INFO: Epoch 016: loss 4.805 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 8.741 | clip 0.8\n",
            "INFO: Epoch 016: valid_loss 4.96 | num_tokens 9.99 | batch_size 500 | valid_perplexity 142\n",
            "INFO: Epoch 017: loss 4.768 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 8.739 | clip 0.8\n",
            "INFO: Epoch 017: valid_loss 4.92 | num_tokens 9.99 | batch_size 500 | valid_perplexity 137\n",
            "INFO: Epoch 018: loss 4.729 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 8.196 | clip 0.775\n",
            "INFO: Epoch 018: valid_loss 4.89 | num_tokens 9.99 | batch_size 500 | valid_perplexity 133\n",
            "INFO: Epoch 019: loss 4.694 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 8.041 | clip 0.75\n",
            "INFO: Epoch 019: valid_loss 4.85 | num_tokens 9.99 | batch_size 500 | valid_perplexity 128\n",
            "INFO: Epoch 020: loss 4.655 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.645 | clip 0.75\n",
            "INFO: Epoch 020: valid_loss 4.82 | num_tokens 9.99 | batch_size 500 | valid_perplexity 124\n",
            "INFO: Epoch 021: loss 4.625 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.45 | clip 0.725\n",
            "INFO: Epoch 021: valid_loss 4.78 | num_tokens 9.99 | batch_size 500 | valid_perplexity 119\n",
            "INFO: Epoch 022: loss 4.589 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.25 | clip 0.7\n",
            "INFO: Epoch 022: valid_loss 4.75 | num_tokens 9.99 | batch_size 500 | valid_perplexity 115\n",
            "INFO: Epoch 023: loss 4.555 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.841 | clip 0.65\n",
            "INFO: Epoch 023: valid_loss 4.72 | num_tokens 9.99 | batch_size 500 | valid_perplexity 112\n",
            "INFO: Epoch 024: loss 4.52 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.585 | clip 0.6\n",
            "INFO: Epoch 024: valid_loss 4.68 | num_tokens 9.99 | batch_size 500 | valid_perplexity 108\n",
            "INFO: Epoch 025: loss 4.484 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.256 | clip 0.55\n",
            "INFO: Epoch 025: valid_loss 4.65 | num_tokens 9.99 | batch_size 500 | valid_perplexity 105\n",
            "INFO: Epoch 026: loss 4.454 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.348 | clip 0.55\n",
            "INFO: Epoch 026: valid_loss 4.62 | num_tokens 9.99 | batch_size 500 | valid_perplexity 102\n",
            "INFO: Epoch 027: loss 4.425 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.028 | clip 0.55\n",
            "INFO: Epoch 027: valid_loss 4.6 | num_tokens 9.99 | batch_size 500 | valid_perplexity 99\n",
            "INFO: Epoch 028: loss 4.394 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6 | clip 0.525\n",
            "INFO: Epoch 028: valid_loss 4.57 | num_tokens 9.99 | batch_size 500 | valid_perplexity 96.5\n",
            "INFO: Epoch 029: loss 4.363 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.697 | clip 0.475\n",
            "INFO: Epoch 029: valid_loss 4.54 | num_tokens 9.99 | batch_size 500 | valid_perplexity 93.8\n",
            "INFO: Epoch 030: loss 4.335 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.762 | clip 0.5\n",
            "INFO: Epoch 030: valid_loss 4.52 | num_tokens 9.99 | batch_size 500 | valid_perplexity 92\n",
            "INFO: Epoch 031: loss 4.303 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.472 | clip 0.475\n",
            "INFO: Epoch 031: valid_loss 4.5 | num_tokens 9.99 | batch_size 500 | valid_perplexity 89.6\n",
            "INFO: Epoch 032: loss 4.28 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.493 | clip 0.5\n",
            "INFO: Epoch 032: valid_loss 4.46 | num_tokens 9.99 | batch_size 500 | valid_perplexity 86.9\n",
            "INFO: Epoch 033: loss 4.251 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.654 | clip 0.475\n",
            "INFO: Epoch 033: valid_loss 4.45 | num_tokens 9.99 | batch_size 500 | valid_perplexity 85.6\n",
            "INFO: Epoch 034: loss 4.232 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.565 | clip 0.5\n",
            "INFO: Epoch 034: valid_loss 4.43 | num_tokens 9.99 | batch_size 500 | valid_perplexity 84.2\n",
            "INFO: Epoch 035: loss 4.206 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.249 | clip 0.475\n",
            "INFO: Epoch 035: valid_loss 4.41 | num_tokens 9.99 | batch_size 500 | valid_perplexity 82.2\n",
            "INFO: Epoch 036: loss 4.182 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.477 | clip 0.525\n",
            "INFO: Epoch 036: valid_loss 4.4 | num_tokens 9.99 | batch_size 500 | valid_perplexity 81.4\n",
            "INFO: Epoch 037: loss 4.162 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.467 | clip 0.5\n",
            "INFO: Epoch 037: valid_loss 4.37 | num_tokens 9.99 | batch_size 500 | valid_perplexity 78.8\n",
            "INFO: Epoch 038: loss 4.129 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.091 | clip 0.5\n",
            "INFO: Epoch 038: valid_loss 4.36 | num_tokens 9.99 | batch_size 500 | valid_perplexity 78.3\n",
            "INFO: Epoch 039: loss 4.118 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.821 | clip 0.55\n",
            "INFO: Epoch 039: valid_loss 4.34 | num_tokens 9.99 | batch_size 500 | valid_perplexity 76.6\n",
            "INFO: Epoch 040: loss 4.099 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.835 | clip 0.55\n",
            "INFO: Epoch 040: valid_loss 4.33 | num_tokens 9.99 | batch_size 500 | valid_perplexity 75.9\n",
            "INFO: Epoch 041: loss 4.08 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.096 | clip 0.55\n",
            "INFO: Epoch 041: valid_loss 4.32 | num_tokens 9.99 | batch_size 500 | valid_perplexity 74.9\n",
            "INFO: Epoch 042: loss 4.063 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.555 | clip 0.55\n",
            "INFO: Epoch 042: valid_loss 4.3 | num_tokens 9.99 | batch_size 500 | valid_perplexity 74.1\n",
            "INFO: Epoch 043: loss 4.044 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.018 | clip 0.525\n",
            "INFO: Epoch 043: valid_loss 4.28 | num_tokens 9.99 | batch_size 500 | valid_perplexity 72.6\n",
            "INFO: Epoch 044: loss 4.022 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.455 | clip 0.55\n",
            "INFO: Epoch 044: valid_loss 4.29 | num_tokens 9.99 | batch_size 500 | valid_perplexity 73.2\n",
            "INFO: Epoch 045: loss 4.016 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.63 | clip 0.5\n",
            "INFO: Epoch 045: valid_loss 4.25 | num_tokens 9.99 | batch_size 500 | valid_perplexity 70.2\n",
            "INFO: Epoch 046: loss 3.987 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.892 | clip 0.55\n",
            "INFO: Epoch 046: valid_loss 4.27 | num_tokens 9.99 | batch_size 500 | valid_perplexity 71.3\n",
            "INFO: Epoch 047: loss 3.983 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.325 | clip 0.55\n",
            "INFO: Epoch 047: valid_loss 4.23 | num_tokens 9.99 | batch_size 500 | valid_perplexity 68.6\n",
            "INFO: Epoch 048: loss 3.954 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.437 | clip 0.5\n",
            "INFO: Epoch 048: valid_loss 4.26 | num_tokens 9.99 | batch_size 500 | valid_perplexity 70.9\n",
            "INFO: Epoch 049: loss 3.959 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.441 | clip 0.65\n",
            "INFO: Epoch 049: valid_loss 4.21 | num_tokens 9.99 | batch_size 500 | valid_perplexity 67.3\n",
            "INFO: Epoch 050: loss 3.924 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.345 | clip 0.55\n",
            "INFO: Epoch 050: valid_loss 4.21 | num_tokens 9.99 | batch_size 500 | valid_perplexity 67.1\n",
            "INFO: Epoch 051: loss 3.913 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.297 | clip 0.525\n",
            "INFO: Epoch 051: valid_loss 4.19 | num_tokens 9.99 | batch_size 500 | valid_perplexity 66\n",
            "INFO: Epoch 052: loss 3.892 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.331 | clip 0.55\n",
            "INFO: Epoch 052: valid_loss 4.19 | num_tokens 9.99 | batch_size 500 | valid_perplexity 65.9\n",
            "INFO: Epoch 053: loss 3.887 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.213 | clip 0.6\n",
            "INFO: Epoch 053: valid_loss 4.17 | num_tokens 9.99 | batch_size 500 | valid_perplexity 64.7\n",
            "INFO: Epoch 054: loss 3.865 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.316 | clip 0.55\n",
            "INFO: Epoch 054: valid_loss 4.16 | num_tokens 9.99 | batch_size 500 | valid_perplexity 64.4\n",
            "INFO: Epoch 055: loss 3.858 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.057 | clip 0.55\n",
            "INFO: Epoch 055: valid_loss 4.15 | num_tokens 9.99 | batch_size 500 | valid_perplexity 63.3\n",
            "INFO: Epoch 056: loss 3.835 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.296 | clip 0.55\n",
            "INFO: Epoch 056: valid_loss 4.15 | num_tokens 9.99 | batch_size 500 | valid_perplexity 63.6\n",
            "INFO: Epoch 057: loss 3.833 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.152 | clip 0.55\n",
            "INFO: Epoch 057: valid_loss 4.13 | num_tokens 9.99 | batch_size 500 | valid_perplexity 62.1\n",
            "INFO: Epoch 058: loss 3.807 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.083 | clip 0.525\n",
            "INFO: Epoch 058: valid_loss 4.13 | num_tokens 9.99 | batch_size 500 | valid_perplexity 62\n",
            "INFO: Epoch 059: loss 3.803 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.442 | clip 0.55\n",
            "INFO: Epoch 059: valid_loss 4.11 | num_tokens 9.99 | batch_size 500 | valid_perplexity 60.9\n",
            "INFO: Epoch 060: loss 3.784 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.217 | clip 0.5\n",
            "INFO: Epoch 060: valid_loss 4.11 | num_tokens 9.99 | batch_size 500 | valid_perplexity 61\n",
            "INFO: Epoch 061: loss 3.777 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.5 | clip 0.6\n",
            "INFO: Epoch 061: valid_loss 4.1 | num_tokens 9.99 | batch_size 500 | valid_perplexity 60\n",
            "INFO: Epoch 062: loss 3.761 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.878 | clip 0.575\n",
            "INFO: Epoch 062: valid_loss 4.1 | num_tokens 9.99 | batch_size 500 | valid_perplexity 60.3\n",
            "INFO: Epoch 063: loss 3.75 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.232 | clip 0.6\n",
            "INFO: Epoch 063: valid_loss 4.08 | num_tokens 9.99 | batch_size 500 | valid_perplexity 59.1\n",
            "INFO: Epoch 064: loss 3.734 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.195 | clip 0.575\n",
            "INFO: Epoch 064: valid_loss 4.07 | num_tokens 9.99 | batch_size 500 | valid_perplexity 58.7\n",
            "INFO: Epoch 065: loss 3.721 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.55 | clip 0.55\n",
            "INFO: Epoch 065: valid_loss 4.06 | num_tokens 9.99 | batch_size 500 | valid_perplexity 57.8\n",
            "INFO: Epoch 066: loss 3.71 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.066 | clip 0.6\n",
            "INFO: Epoch 066: valid_loss 4.08 | num_tokens 9.99 | batch_size 500 | valid_perplexity 58.9\n",
            "INFO: Epoch 067: loss 3.71 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.393 | clip 0.575\n",
            "INFO: Epoch 067: valid_loss 4.05 | num_tokens 9.99 | batch_size 500 | valid_perplexity 57.2\n",
            "INFO: Epoch 068: loss 3.685 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.515 | clip 0.575\n",
            "INFO: Epoch 068: valid_loss 4.05 | num_tokens 9.99 | batch_size 500 | valid_perplexity 57.6\n",
            "INFO: Epoch 069: loss 3.68 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.444 | clip 0.6\n",
            "INFO: Epoch 069: valid_loss 4.04 | num_tokens 9.99 | batch_size 500 | valid_perplexity 56.7\n",
            "INFO: Epoch 070: loss 3.662 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.63 | clip 0.55\n",
            "INFO: Epoch 070: valid_loss 4.03 | num_tokens 9.99 | batch_size 500 | valid_perplexity 56.1\n",
            "INFO: Epoch 071: loss 3.654 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.417 | clip 0.625\n",
            "INFO: Epoch 071: valid_loss 4.02 | num_tokens 9.99 | batch_size 500 | valid_perplexity 55.5\n",
            "INFO: Epoch 072: loss 3.641 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.784 | clip 0.575\n",
            "INFO: Epoch 072: valid_loss 4.01 | num_tokens 9.99 | batch_size 500 | valid_perplexity 55\n",
            "INFO: Epoch 073: loss 3.631 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.936 | clip 0.6\n",
            "INFO: Epoch 073: valid_loss 4 | num_tokens 9.99 | batch_size 500 | valid_perplexity 54.5\n",
            "INFO: Epoch 074: loss 3.614 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.118 | clip 0.6\n",
            "INFO: Epoch 074: valid_loss 3.99 | num_tokens 9.99 | batch_size 500 | valid_perplexity 54.3\n",
            "INFO: Epoch 075: loss 3.604 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.709 | clip 0.6\n",
            "INFO: Epoch 075: valid_loss 3.99 | num_tokens 9.99 | batch_size 500 | valid_perplexity 54\n",
            "INFO: Epoch 076: loss 3.595 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.733 | clip 0.625\n",
            "INFO: Epoch 076: valid_loss 3.97 | num_tokens 9.99 | batch_size 500 | valid_perplexity 52.9\n",
            "INFO: Epoch 077: loss 3.579 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.595 | clip 0.6\n",
            "INFO: Epoch 077: valid_loss 3.97 | num_tokens 9.99 | batch_size 500 | valid_perplexity 53.2\n",
            "INFO: Epoch 078: loss 3.57 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.063 | clip 0.575\n",
            "INFO: Epoch 078: valid_loss 3.95 | num_tokens 9.99 | batch_size 500 | valid_perplexity 52\n",
            "INFO: Epoch 079: loss 3.559 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.642 | clip 0.575\n",
            "INFO: Epoch 079: valid_loss 3.95 | num_tokens 9.99 | batch_size 500 | valid_perplexity 52.1\n",
            "INFO: Epoch 080: loss 3.548 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.846 | clip 0.6\n",
            "INFO: Epoch 080: valid_loss 3.94 | num_tokens 9.99 | batch_size 500 | valid_perplexity 51.6\n",
            "INFO: Epoch 081: loss 3.533 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.403 | clip 0.575\n",
            "INFO: Epoch 081: valid_loss 3.93 | num_tokens 9.99 | batch_size 500 | valid_perplexity 50.9\n",
            "INFO: Epoch 082: loss 3.519 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.431 | clip 0.55\n",
            "INFO: Epoch 082: valid_loss 3.93 | num_tokens 9.99 | batch_size 500 | valid_perplexity 50.8\n",
            "INFO: Epoch 083: loss 3.516 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.625 | clip 0.625\n",
            "INFO: Epoch 083: valid_loss 3.92 | num_tokens 9.99 | batch_size 500 | valid_perplexity 50.2\n",
            "INFO: Epoch 084: loss 3.494 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.659 | clip 0.575\n",
            "INFO: Epoch 084: valid_loss 3.91 | num_tokens 9.99 | batch_size 500 | valid_perplexity 49.9\n",
            "INFO: Epoch 085: loss 3.49 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.658 | clip 0.575\n",
            "INFO: Epoch 085: valid_loss 3.89 | num_tokens 9.99 | batch_size 500 | valid_perplexity 49.1\n",
            "INFO: Epoch 086: loss 3.47 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.721 | clip 0.525\n",
            "INFO: Epoch 086: valid_loss 3.9 | num_tokens 9.99 | batch_size 500 | valid_perplexity 49.4\n",
            "INFO: Epoch 087: loss 3.469 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.374 | clip 0.6\n",
            "INFO: Epoch 087: valid_loss 3.88 | num_tokens 9.99 | batch_size 500 | valid_perplexity 48.2\n",
            "INFO: Epoch 088: loss 3.448 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.658 | clip 0.55\n",
            "INFO: Epoch 088: valid_loss 3.88 | num_tokens 9.99 | batch_size 500 | valid_perplexity 48.3\n",
            "INFO: Epoch 089: loss 3.447 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.475 | clip 0.6\n",
            "INFO: Epoch 089: valid_loss 3.87 | num_tokens 9.99 | batch_size 500 | valid_perplexity 47.9\n",
            "INFO: Epoch 090: loss 3.427 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.103 | clip 0.6\n",
            "INFO: Epoch 090: valid_loss 3.86 | num_tokens 9.99 | batch_size 500 | valid_perplexity 47.6\n",
            "INFO: Epoch 091: loss 3.42 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.976 | clip 0.6\n",
            "INFO: Epoch 091: valid_loss 3.84 | num_tokens 9.99 | batch_size 500 | valid_perplexity 46.7\n",
            "INFO: Epoch 092: loss 3.402 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.378 | clip 0.575\n",
            "INFO: Epoch 092: valid_loss 3.84 | num_tokens 9.99 | batch_size 500 | valid_perplexity 46.5\n",
            "INFO: Epoch 093: loss 3.393 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.639 | clip 0.6\n",
            "INFO: Epoch 093: valid_loss 3.83 | num_tokens 9.99 | batch_size 500 | valid_perplexity 46\n",
            "INFO: Epoch 094: loss 3.383 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.943 | clip 0.55\n",
            "INFO: Epoch 094: valid_loss 3.83 | num_tokens 9.99 | batch_size 500 | valid_perplexity 45.9\n",
            "INFO: Epoch 095: loss 3.375 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.446 | clip 0.575\n",
            "INFO: Epoch 095: valid_loss 3.82 | num_tokens 9.99 | batch_size 500 | valid_perplexity 45.4\n",
            "INFO: Epoch 096: loss 3.355 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.477 | clip 0.575\n",
            "INFO: Epoch 096: valid_loss 3.81 | num_tokens 9.99 | batch_size 500 | valid_perplexity 45\n",
            "INFO: Epoch 097: loss 3.354 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.734 | clip 0.6\n",
            "INFO: Epoch 097: valid_loss 3.8 | num_tokens 9.99 | batch_size 500 | valid_perplexity 44.6\n",
            "INFO: Epoch 098: loss 3.335 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.97 | clip 0.525\n",
            "INFO: Epoch 098: valid_loss 3.79 | num_tokens 9.99 | batch_size 500 | valid_perplexity 44.5\n",
            "INFO: Epoch 099: loss 3.329 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.438 | clip 0.65\n",
            "INFO: Epoch 099: valid_loss 3.78 | num_tokens 9.99 | batch_size 500 | valid_perplexity 43.8\n",
            "INFO: Epoch 100: loss 3.311 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.69 | clip 0.575\n",
            "INFO: Epoch 100: valid_loss 3.78 | num_tokens 9.99 | batch_size 500 | valid_perplexity 44\n",
            "INFO: Epoch 101: loss 3.311 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.562 | clip 0.65\n",
            "INFO: Epoch 101: valid_loss 3.77 | num_tokens 9.99 | batch_size 500 | valid_perplexity 43.3\n",
            "INFO: Epoch 102: loss 3.293 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.704 | clip 0.575\n",
            "INFO: Epoch 102: valid_loss 3.77 | num_tokens 9.99 | batch_size 500 | valid_perplexity 43.4\n",
            "INFO: Epoch 103: loss 3.287 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.265 | clip 0.55\n",
            "INFO: Epoch 103: valid_loss 3.75 | num_tokens 9.99 | batch_size 500 | valid_perplexity 42.5\n",
            "INFO: Epoch 104: loss 3.265 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.032 | clip 0.55\n",
            "INFO: Epoch 104: valid_loss 3.75 | num_tokens 9.99 | batch_size 500 | valid_perplexity 42.7\n",
            "INFO: Epoch 105: loss 3.265 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.385 | clip 0.6\n",
            "INFO: Epoch 105: valid_loss 3.73 | num_tokens 9.99 | batch_size 500 | valid_perplexity 41.8\n",
            "INFO: Epoch 106: loss 3.247 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.786 | clip 0.55\n",
            "INFO: Epoch 106: valid_loss 3.74 | num_tokens 9.99 | batch_size 500 | valid_perplexity 42.2\n",
            "INFO: Epoch 107: loss 3.244 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.508 | clip 0.575\n",
            "INFO: Epoch 107: valid_loss 3.72 | num_tokens 9.99 | batch_size 500 | valid_perplexity 41.3\n",
            "INFO: Epoch 108: loss 3.226 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.248 | clip 0.55\n",
            "INFO: Epoch 108: valid_loss 3.73 | num_tokens 9.99 | batch_size 500 | valid_perplexity 41.9\n",
            "INFO: Epoch 109: loss 3.225 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.626 | clip 0.625\n",
            "INFO: Epoch 109: valid_loss 3.71 | num_tokens 9.99 | batch_size 500 | valid_perplexity 40.8\n",
            "INFO: Epoch 110: loss 3.205 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.227 | clip 0.525\n",
            "INFO: Epoch 110: valid_loss 3.72 | num_tokens 9.99 | batch_size 500 | valid_perplexity 41.2\n",
            "INFO: Epoch 111: loss 3.206 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.067 | clip 0.65\n",
            "INFO: Epoch 111: valid_loss 3.69 | num_tokens 9.99 | batch_size 500 | valid_perplexity 40.2\n",
            "INFO: Epoch 112: loss 3.186 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.255 | clip 0.55\n",
            "INFO: Epoch 112: valid_loss 3.71 | num_tokens 9.99 | batch_size 500 | valid_perplexity 40.7\n",
            "INFO: Epoch 113: loss 3.184 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.937 | clip 0.6\n",
            "INFO: Epoch 113: valid_loss 3.68 | num_tokens 9.99 | batch_size 500 | valid_perplexity 39.7\n",
            "INFO: Epoch 114: loss 3.164 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.164 | clip 0.525\n",
            "INFO: Epoch 114: valid_loss 3.69 | num_tokens 9.99 | batch_size 500 | valid_perplexity 40.1\n",
            "INFO: Epoch 115: loss 3.165 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.729 | clip 0.625\n",
            "INFO: Epoch 115: valid_loss 3.67 | num_tokens 9.99 | batch_size 500 | valid_perplexity 39.3\n",
            "INFO: Epoch 116: loss 3.144 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.908 | clip 0.55\n",
            "INFO: Epoch 116: valid_loss 3.66 | num_tokens 9.99 | batch_size 500 | valid_perplexity 39\n",
            "INFO: Epoch 117: loss 3.141 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.472 | clip 0.625\n",
            "INFO: Epoch 117: valid_loss 3.66 | num_tokens 9.99 | batch_size 500 | valid_perplexity 39\n",
            "INFO: Epoch 118: loss 3.128 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.659 | clip 0.5\n",
            "INFO: Epoch 118: valid_loss 3.65 | num_tokens 9.99 | batch_size 500 | valid_perplexity 38.5\n",
            "INFO: Epoch 119: loss 3.119 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.448 | clip 0.55\n",
            "INFO: Epoch 119: valid_loss 3.66 | num_tokens 9.99 | batch_size 500 | valid_perplexity 38.7\n",
            "INFO: Epoch 120: loss 3.112 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.887 | clip 0.575\n",
            "INFO: Epoch 120: valid_loss 3.64 | num_tokens 9.99 | batch_size 500 | valid_perplexity 38.2\n",
            "INFO: Epoch 121: loss 3.1 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.394 | clip 0.525\n",
            "INFO: Epoch 121: valid_loss 3.64 | num_tokens 9.99 | batch_size 500 | valid_perplexity 37.9\n",
            "INFO: Epoch 122: loss 3.086 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.617 | clip 0.55\n",
            "INFO: Epoch 122: valid_loss 3.63 | num_tokens 9.99 | batch_size 500 | valid_perplexity 37.8\n",
            "INFO: Epoch 123: loss 3.08 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.583 | clip 0.6\n",
            "INFO: Epoch 123: valid_loss 3.62 | num_tokens 9.99 | batch_size 500 | valid_perplexity 37.5\n",
            "INFO: Epoch 124: loss 3.068 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.566 | clip 0.6\n",
            "INFO: Epoch 124: valid_loss 3.61 | num_tokens 9.99 | batch_size 500 | valid_perplexity 37.1\n",
            "INFO: Epoch 125: loss 3.065 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.654 | clip 0.6\n",
            "INFO: Epoch 125: valid_loss 3.61 | num_tokens 9.99 | batch_size 500 | valid_perplexity 37.1\n",
            "INFO: Epoch 126: loss 3.048 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.089 | clip 0.55\n",
            "INFO: Epoch 126: valid_loss 3.61 | num_tokens 9.99 | batch_size 500 | valid_perplexity 37\n",
            "INFO: Epoch 127: loss 3.045 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.755 | clip 0.625\n",
            "INFO: Epoch 127: valid_loss 3.6 | num_tokens 9.99 | batch_size 500 | valid_perplexity 36.7\n",
            "INFO: Epoch 128: loss 3.029 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.482 | clip 0.55\n",
            "INFO: Epoch 128: valid_loss 3.61 | num_tokens 9.99 | batch_size 500 | valid_perplexity 37.1\n",
            "INFO: Epoch 129: loss 3.035 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.014 | clip 0.675\n",
            "INFO: Epoch 129: valid_loss 3.59 | num_tokens 9.99 | batch_size 500 | valid_perplexity 36.1\n",
            "INFO: Epoch 130: loss 3.011 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.037 | clip 0.525\n",
            "INFO: Epoch 130: valid_loss 3.6 | num_tokens 9.99 | batch_size 500 | valid_perplexity 36.5\n",
            "INFO: Epoch 131: loss 3.012 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.593 | clip 0.625\n",
            "INFO: Epoch 131: valid_loss 3.58 | num_tokens 9.99 | batch_size 500 | valid_perplexity 35.7\n",
            "INFO: Epoch 132: loss 2.994 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.798 | clip 0.525\n",
            "INFO: Epoch 132: valid_loss 3.59 | num_tokens 9.99 | batch_size 500 | valid_perplexity 36.2\n",
            "INFO: Epoch 133: loss 2.992 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.641 | clip 0.65\n",
            "INFO: Epoch 133: valid_loss 3.56 | num_tokens 9.99 | batch_size 500 | valid_perplexity 35.3\n",
            "INFO: Epoch 134: loss 2.978 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.185 | clip 0.55\n",
            "INFO: Epoch 134: valid_loss 3.57 | num_tokens 9.99 | batch_size 500 | valid_perplexity 35.5\n",
            "INFO: Epoch 135: loss 2.975 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.984 | clip 0.65\n",
            "INFO: Epoch 135: valid_loss 3.55 | num_tokens 9.99 | batch_size 500 | valid_perplexity 34.9\n",
            "INFO: Epoch 136: loss 2.956 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.216 | clip 0.6\n",
            "INFO: Epoch 136: valid_loss 3.57 | num_tokens 9.99 | batch_size 500 | valid_perplexity 35.6\n",
            "INFO: Epoch 137: loss 2.962 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.324 | clip 0.675\n",
            "INFO: Epoch 137: valid_loss 3.54 | num_tokens 9.99 | batch_size 500 | valid_perplexity 34.5\n",
            "INFO: Epoch 138: loss 2.943 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.063 | clip 0.525\n",
            "INFO: Epoch 138: valid_loss 3.56 | num_tokens 9.99 | batch_size 500 | valid_perplexity 35.2\n",
            "INFO: Epoch 139: loss 2.941 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.87 | clip 0.7\n",
            "INFO: Epoch 139: valid_loss 3.53 | num_tokens 9.99 | batch_size 500 | valid_perplexity 34.2\n",
            "INFO: Epoch 140: loss 2.926 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.718 | clip 0.525\n",
            "INFO: Epoch 140: valid_loss 3.54 | num_tokens 9.99 | batch_size 500 | valid_perplexity 34.5\n",
            "INFO: Epoch 141: loss 2.927 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.801 | clip 0.675\n",
            "INFO: Epoch 141: valid_loss 3.53 | num_tokens 9.99 | batch_size 500 | valid_perplexity 34\n",
            "INFO: Epoch 142: loss 2.913 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.226 | clip 0.55\n",
            "INFO: Epoch 142: valid_loss 3.53 | num_tokens 9.99 | batch_size 500 | valid_perplexity 34.1\n",
            "INFO: Epoch 143: loss 2.904 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.23 | clip 0.625\n",
            "INFO: Epoch 143: valid_loss 3.52 | num_tokens 9.99 | batch_size 500 | valid_perplexity 33.6\n",
            "INFO: Epoch 144: loss 2.889 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.964 | clip 0.6\n",
            "INFO: Epoch 144: valid_loss 3.52 | num_tokens 9.99 | batch_size 500 | valid_perplexity 33.7\n",
            "INFO: Epoch 145: loss 2.886 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.108 | clip 0.65\n",
            "INFO: Epoch 145: valid_loss 3.51 | num_tokens 9.99 | batch_size 500 | valid_perplexity 33.4\n",
            "INFO: Epoch 146: loss 2.876 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.762 | clip 0.55\n",
            "INFO: Epoch 146: valid_loss 3.51 | num_tokens 9.99 | batch_size 500 | valid_perplexity 33.6\n",
            "INFO: Epoch 147: loss 2.872 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.78 | clip 0.675\n",
            "INFO: Epoch 147: valid_loss 3.5 | num_tokens 9.99 | batch_size 500 | valid_perplexity 33.1\n",
            "INFO: Epoch 148: loss 2.86 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.794 | clip 0.55\n",
            "INFO: Epoch 148: valid_loss 3.5 | num_tokens 9.99 | batch_size 500 | valid_perplexity 33\n",
            "INFO: Epoch 149: loss 2.856 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.917 | clip 0.625\n",
            "INFO: Epoch 149: valid_loss 3.49 | num_tokens 9.99 | batch_size 500 | valid_perplexity 32.8\n",
            "INFO: Epoch 150: loss 2.84 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.116 | clip 0.525\n",
            "INFO: Epoch 150: valid_loss 3.49 | num_tokens 9.99 | batch_size 500 | valid_perplexity 32.8\n",
            "INFO: Epoch 151: loss 2.842 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.22 | clip 0.675\n",
            "INFO: Epoch 151: valid_loss 3.48 | num_tokens 9.99 | batch_size 500 | valid_perplexity 32.4\n",
            "INFO: Epoch 152: loss 2.829 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.707 | clip 0.6\n",
            "INFO: Epoch 152: valid_loss 3.49 | num_tokens 9.99 | batch_size 500 | valid_perplexity 32.9\n",
            "INFO: Epoch 153: loss 2.831 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.122 | clip 0.675\n",
            "INFO: Epoch 153: valid_loss 3.47 | num_tokens 9.99 | batch_size 500 | valid_perplexity 32.2\n",
            "INFO: Epoch 154: loss 2.81 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.247 | clip 0.575\n",
            "INFO: Epoch 154: valid_loss 3.48 | num_tokens 9.99 | batch_size 500 | valid_perplexity 32.5\n",
            "INFO: Epoch 155: loss 2.81 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.948 | clip 0.65\n",
            "INFO: Epoch 155: valid_loss 3.46 | num_tokens 9.99 | batch_size 500 | valid_perplexity 31.9\n",
            "INFO: Epoch 156: loss 2.795 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.367 | clip 0.55\n",
            "INFO: Epoch 156: valid_loss 3.47 | num_tokens 9.99 | batch_size 500 | valid_perplexity 32\n",
            "INFO: Epoch 157: loss 2.796 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.895 | clip 0.65\n",
            "INFO: Epoch 157: valid_loss 3.45 | num_tokens 9.99 | batch_size 500 | valid_perplexity 31.6\n",
            "INFO: Epoch 158: loss 2.776 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.431 | clip 0.575\n",
            "INFO: Epoch 158: valid_loss 3.46 | num_tokens 9.99 | batch_size 500 | valid_perplexity 31.8\n",
            "INFO: Epoch 159: loss 2.778 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.226 | clip 0.725\n",
            "INFO: Epoch 159: valid_loss 3.44 | num_tokens 9.99 | batch_size 500 | valid_perplexity 31.3\n",
            "INFO: Epoch 160: loss 2.762 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.583 | clip 0.55\n",
            "INFO: Epoch 160: valid_loss 3.46 | num_tokens 9.99 | batch_size 500 | valid_perplexity 31.8\n",
            "INFO: Epoch 161: loss 2.766 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.397 | clip 0.725\n",
            "INFO: Epoch 161: valid_loss 3.43 | num_tokens 9.99 | batch_size 500 | valid_perplexity 31\n",
            "INFO: Epoch 162: loss 2.752 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.815 | clip 0.5\n",
            "INFO: Epoch 162: valid_loss 3.45 | num_tokens 9.99 | batch_size 500 | valid_perplexity 31.4\n",
            "INFO: Epoch 163: loss 2.749 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.799 | clip 0.675\n",
            "INFO: Epoch 163: valid_loss 3.42 | num_tokens 9.99 | batch_size 500 | valid_perplexity 30.7\n",
            "INFO: Epoch 164: loss 2.73 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.083 | clip 0.525\n",
            "INFO: Epoch 164: valid_loss 3.43 | num_tokens 9.99 | batch_size 500 | valid_perplexity 30.8\n",
            "INFO: Epoch 165: loss 2.73 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.882 | clip 0.675\n",
            "INFO: Epoch 165: valid_loss 3.42 | num_tokens 9.99 | batch_size 500 | valid_perplexity 30.5\n",
            "INFO: Epoch 166: loss 2.714 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.768 | clip 0.575\n",
            "INFO: Epoch 166: valid_loss 3.42 | num_tokens 9.99 | batch_size 500 | valid_perplexity 30.7\n",
            "INFO: Epoch 167: loss 2.715 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.961 | clip 0.65\n",
            "INFO: Epoch 167: valid_loss 3.41 | num_tokens 9.99 | batch_size 500 | valid_perplexity 30.3\n",
            "INFO: Epoch 168: loss 2.701 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.486 | clip 0.625\n",
            "INFO: Epoch 168: valid_loss 3.43 | num_tokens 9.99 | batch_size 500 | valid_perplexity 30.8\n",
            "INFO: Epoch 169: loss 2.706 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.103 | clip 0.725\n",
            "INFO: Epoch 169: valid_loss 3.4 | num_tokens 9.99 | batch_size 500 | valid_perplexity 30.1\n",
            "INFO: Epoch 170: loss 2.688 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.291 | clip 0.625\n",
            "INFO: Epoch 170: valid_loss 3.41 | num_tokens 9.99 | batch_size 500 | valid_perplexity 30.4\n",
            "INFO: Epoch 171: loss 2.69 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.2 | clip 0.675\n",
            "INFO: Epoch 171: valid_loss 3.39 | num_tokens 9.99 | batch_size 500 | valid_perplexity 29.8\n",
            "INFO: Epoch 172: loss 2.67 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.926 | clip 0.575\n",
            "INFO: Epoch 172: valid_loss 3.41 | num_tokens 9.99 | batch_size 500 | valid_perplexity 30.4\n",
            "INFO: Epoch 173: loss 2.678 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.326 | clip 0.725\n",
            "INFO: Epoch 173: valid_loss 3.39 | num_tokens 9.99 | batch_size 500 | valid_perplexity 29.6\n",
            "INFO: Epoch 174: loss 2.661 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.38 | clip 0.6\n",
            "INFO: Epoch 174: valid_loss 3.4 | num_tokens 9.99 | batch_size 500 | valid_perplexity 30\n",
            "INFO: Epoch 175: loss 2.661 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.291 | clip 0.65\n",
            "INFO: Epoch 175: valid_loss 3.38 | num_tokens 9.99 | batch_size 500 | valid_perplexity 29.4\n",
            "INFO: Epoch 176: loss 2.648 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.867 | clip 0.55\n",
            "INFO: Epoch 176: valid_loss 3.39 | num_tokens 9.99 | batch_size 500 | valid_perplexity 29.8\n",
            "INFO: Epoch 177: loss 2.646 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.859 | clip 0.725\n",
            "INFO: Epoch 177: valid_loss 3.38 | num_tokens 9.99 | batch_size 500 | valid_perplexity 29.3\n",
            "INFO: Epoch 178: loss 2.634 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.377 | clip 0.6\n",
            "INFO: Epoch 178: valid_loss 3.38 | num_tokens 9.99 | batch_size 500 | valid_perplexity 29.4\n",
            "INFO: Epoch 179: loss 2.631 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.344 | clip 0.675\n",
            "INFO: Epoch 179: valid_loss 3.37 | num_tokens 9.99 | batch_size 500 | valid_perplexity 29.2\n",
            "INFO: Epoch 180: loss 2.62 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.47 | clip 0.65\n",
            "INFO: Epoch 180: valid_loss 3.37 | num_tokens 9.99 | batch_size 500 | valid_perplexity 29.1\n",
            "INFO: Epoch 181: loss 2.619 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.091 | clip 0.675\n",
            "INFO: Epoch 181: valid_loss 3.37 | num_tokens 9.99 | batch_size 500 | valid_perplexity 29.1\n",
            "INFO: Epoch 182: loss 2.61 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.001 | clip 0.625\n",
            "INFO: Epoch 182: valid_loss 3.36 | num_tokens 9.99 | batch_size 500 | valid_perplexity 28.8\n",
            "INFO: Epoch 183: loss 2.6 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.83 | clip 0.65\n",
            "INFO: Epoch 183: valid_loss 3.36 | num_tokens 9.99 | batch_size 500 | valid_perplexity 28.8\n",
            "INFO: Epoch 184: loss 2.593 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 5.957 | clip 0.575\n",
            "INFO: Epoch 184: valid_loss 3.36 | num_tokens 9.99 | batch_size 500 | valid_perplexity 28.8\n",
            "INFO: Epoch 185: loss 2.583 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.031 | clip 0.7\n",
            "INFO: Epoch 185: valid_loss 3.35 | num_tokens 9.99 | batch_size 500 | valid_perplexity 28.5\n",
            "INFO: Epoch 186: loss 2.578 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.37 | clip 0.6\n",
            "INFO: Epoch 186: valid_loss 3.36 | num_tokens 9.99 | batch_size 500 | valid_perplexity 28.7\n",
            "INFO: Epoch 187: loss 2.576 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.748 | clip 0.65\n",
            "INFO: Epoch 187: valid_loss 3.35 | num_tokens 9.99 | batch_size 500 | valid_perplexity 28.4\n",
            "INFO: Epoch 188: loss 2.564 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.369 | clip 0.6\n",
            "INFO: Epoch 188: valid_loss 3.36 | num_tokens 9.99 | batch_size 500 | valid_perplexity 28.9\n",
            "INFO: Epoch 189: loss 2.565 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.614 | clip 0.775\n",
            "INFO: Epoch 189: valid_loss 3.34 | num_tokens 9.99 | batch_size 500 | valid_perplexity 28.2\n",
            "INFO: Epoch 190: loss 2.55 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.861 | clip 0.575\n",
            "INFO: Epoch 190: valid_loss 3.36 | num_tokens 9.99 | batch_size 500 | valid_perplexity 28.7\n",
            "INFO: Epoch 191: loss 2.552 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.643 | clip 0.75\n",
            "INFO: Epoch 191: valid_loss 3.33 | num_tokens 9.99 | batch_size 500 | valid_perplexity 28\n",
            "INFO: Epoch 192: loss 2.538 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.415 | clip 0.575\n",
            "INFO: Epoch 192: valid_loss 3.35 | num_tokens 9.99 | batch_size 500 | valid_perplexity 28.5\n",
            "INFO: Epoch 193: loss 2.538 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.191 | clip 0.7\n",
            "INFO: Epoch 193: valid_loss 3.32 | num_tokens 9.99 | batch_size 500 | valid_perplexity 27.8\n",
            "INFO: Epoch 194: loss 2.526 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.585 | clip 0.65\n",
            "INFO: Epoch 194: valid_loss 3.34 | num_tokens 9.99 | batch_size 500 | valid_perplexity 28.1\n",
            "INFO: Epoch 195: loss 2.523 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.13 | clip 0.675\n",
            "INFO: Epoch 195: valid_loss 3.32 | num_tokens 9.99 | batch_size 500 | valid_perplexity 27.7\n",
            "INFO: Epoch 196: loss 2.509 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.471 | clip 0.625\n",
            "INFO: Epoch 196: valid_loss 3.33 | num_tokens 9.99 | batch_size 500 | valid_perplexity 27.8\n",
            "INFO: Epoch 197: loss 2.507 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.239 | clip 0.625\n",
            "INFO: Epoch 197: valid_loss 3.32 | num_tokens 9.99 | batch_size 500 | valid_perplexity 27.7\n",
            "INFO: Epoch 198: loss 2.501 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.246 | clip 0.625\n",
            "INFO: Epoch 198: valid_loss 3.32 | num_tokens 9.99 | batch_size 500 | valid_perplexity 27.8\n",
            "INFO: Epoch 199: loss 2.495 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.25 | clip 0.725\n",
            "INFO: Epoch 199: valid_loss 3.31 | num_tokens 9.99 | batch_size 500 | valid_perplexity 27.4\n",
            "INFO: Epoch 200: loss 2.487 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.502 | clip 0.675\n",
            "INFO: Epoch 200: valid_loss 3.32 | num_tokens 9.99 | batch_size 500 | valid_perplexity 27.7\n",
            "INFO: Epoch 201: loss 2.481 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.528 | clip 0.675\n",
            "INFO: Epoch 201: valid_loss 3.31 | num_tokens 9.99 | batch_size 500 | valid_perplexity 27.3\n",
            "INFO: Epoch 202: loss 2.469 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.547 | clip 0.65\n",
            "INFO: Epoch 202: valid_loss 3.31 | num_tokens 9.99 | batch_size 500 | valid_perplexity 27.5\n",
            "INFO: Epoch 203: loss 2.47 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.574 | clip 0.725\n",
            "INFO: Epoch 203: valid_loss 3.3 | num_tokens 9.99 | batch_size 500 | valid_perplexity 27.1\n",
            "INFO: Epoch 204: loss 2.463 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.865 | clip 0.675\n",
            "INFO: Epoch 204: valid_loss 3.31 | num_tokens 9.99 | batch_size 500 | valid_perplexity 27.3\n",
            "INFO: Epoch 205: loss 2.459 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.643 | clip 0.675\n",
            "INFO: Epoch 205: valid_loss 3.3 | num_tokens 9.99 | batch_size 500 | valid_perplexity 27\n",
            "INFO: Epoch 206: loss 2.446 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.31 | clip 0.625\n",
            "INFO: Epoch 206: valid_loss 3.3 | num_tokens 9.99 | batch_size 500 | valid_perplexity 27.2\n",
            "INFO: Epoch 207: loss 2.448 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.453 | clip 0.725\n",
            "INFO: Epoch 207: valid_loss 3.29 | num_tokens 9.99 | batch_size 500 | valid_perplexity 26.7\n",
            "INFO: Epoch 208: loss 2.435 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.031 | clip 0.675\n",
            "INFO: Epoch 208: valid_loss 3.31 | num_tokens 9.99 | batch_size 500 | valid_perplexity 27.3\n",
            "INFO: Epoch 209: loss 2.441 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.503 | clip 0.75\n",
            "INFO: Epoch 209: valid_loss 3.29 | num_tokens 9.99 | batch_size 500 | valid_perplexity 26.7\n",
            "INFO: Epoch 210: loss 2.424 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.736 | clip 0.65\n",
            "INFO: Epoch 210: valid_loss 3.3 | num_tokens 9.99 | batch_size 500 | valid_perplexity 27\n",
            "INFO: Epoch 211: loss 2.42 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.467 | clip 0.75\n",
            "INFO: Epoch 211: valid_loss 3.28 | num_tokens 9.99 | batch_size 500 | valid_perplexity 26.5\n",
            "INFO: Epoch 212: loss 2.414 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.319 | clip 0.675\n",
            "INFO: Epoch 212: valid_loss 3.3 | num_tokens 9.99 | batch_size 500 | valid_perplexity 27.1\n",
            "INFO: Epoch 213: loss 2.415 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.187 | clip 0.85\n",
            "INFO: Epoch 213: valid_loss 3.28 | num_tokens 9.99 | batch_size 500 | valid_perplexity 26.5\n",
            "INFO: Epoch 214: loss 2.399 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.399 | clip 0.7\n",
            "INFO: Epoch 214: valid_loss 3.29 | num_tokens 9.99 | batch_size 500 | valid_perplexity 26.9\n",
            "INFO: Epoch 215: loss 2.401 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.598 | clip 0.75\n",
            "INFO: Epoch 215: valid_loss 3.27 | num_tokens 9.99 | batch_size 500 | valid_perplexity 26.3\n",
            "INFO: Epoch 216: loss 2.387 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.697 | clip 0.65\n",
            "INFO: Epoch 216: valid_loss 3.28 | num_tokens 9.99 | batch_size 500 | valid_perplexity 26.5\n",
            "INFO: Epoch 217: loss 2.386 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.61 | clip 0.725\n",
            "INFO: Epoch 217: valid_loss 3.27 | num_tokens 9.99 | batch_size 500 | valid_perplexity 26.2\n",
            "INFO: Epoch 218: loss 2.376 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.833 | clip 0.7\n",
            "INFO: Epoch 218: valid_loss 3.28 | num_tokens 9.99 | batch_size 500 | valid_perplexity 26.6\n",
            "INFO: Epoch 219: loss 2.375 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.578 | clip 0.75\n",
            "INFO: Epoch 219: valid_loss 3.26 | num_tokens 9.99 | batch_size 500 | valid_perplexity 26.1\n",
            "INFO: Epoch 220: loss 2.364 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.215 | clip 0.675\n",
            "INFO: Epoch 220: valid_loss 3.28 | num_tokens 9.99 | batch_size 500 | valid_perplexity 26.5\n",
            "INFO: Epoch 221: loss 2.366 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.697 | clip 0.725\n",
            "INFO: Epoch 221: valid_loss 3.25 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25.9\n",
            "INFO: Epoch 222: loss 2.351 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 8.23 | clip 0.625\n",
            "INFO: Epoch 222: valid_loss 3.27 | num_tokens 9.99 | batch_size 500 | valid_perplexity 26.4\n",
            "INFO: Epoch 223: loss 2.354 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.158 | clip 0.725\n",
            "INFO: Epoch 223: valid_loss 3.25 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25.7\n",
            "INFO: Epoch 224: loss 2.343 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.443 | clip 0.625\n",
            "INFO: Epoch 224: valid_loss 3.26 | num_tokens 9.99 | batch_size 500 | valid_perplexity 26\n",
            "INFO: Epoch 225: loss 2.341 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.428 | clip 0.7\n",
            "INFO: Epoch 225: valid_loss 3.24 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25.6\n",
            "INFO: Epoch 226: loss 2.328 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.147 | clip 0.65\n",
            "INFO: Epoch 226: valid_loss 3.26 | num_tokens 9.99 | batch_size 500 | valid_perplexity 26.1\n",
            "INFO: Epoch 227: loss 2.33 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.691 | clip 0.725\n",
            "INFO: Epoch 227: valid_loss 3.24 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25.5\n",
            "INFO: Epoch 228: loss 2.318 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.284 | clip 0.7\n",
            "INFO: Epoch 228: valid_loss 3.25 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25.7\n",
            "INFO: Epoch 229: loss 2.314 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.506 | clip 0.7\n",
            "INFO: Epoch 229: valid_loss 3.23 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25.4\n",
            "INFO: Epoch 230: loss 2.304 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.922 | clip 0.725\n",
            "INFO: Epoch 230: valid_loss 3.26 | num_tokens 9.99 | batch_size 500 | valid_perplexity 26\n",
            "INFO: Epoch 231: loss 2.306 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.833 | clip 0.725\n",
            "INFO: Epoch 231: valid_loss 3.23 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25.3\n",
            "INFO: Epoch 232: loss 2.293 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.589 | clip 0.675\n",
            "INFO: Epoch 232: valid_loss 3.24 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25.6\n",
            "INFO: Epoch 233: loss 2.292 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.714 | clip 0.725\n",
            "INFO: Epoch 233: valid_loss 3.23 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25.2\n",
            "INFO: Epoch 234: loss 2.284 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 8.085 | clip 0.725\n",
            "INFO: Epoch 234: valid_loss 3.24 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25.4\n",
            "INFO: Epoch 235: loss 2.286 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.662 | clip 0.7\n",
            "INFO: Epoch 235: valid_loss 3.22 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25.1\n",
            "INFO: Epoch 236: loss 2.275 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.289 | clip 0.7\n",
            "INFO: Epoch 236: valid_loss 3.23 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25.2\n",
            "INFO: Epoch 237: loss 2.269 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.907 | clip 0.725\n",
            "INFO: Epoch 237: valid_loss 3.22 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25\n",
            "INFO: Epoch 238: loss 2.266 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 8.643 | clip 0.675\n",
            "INFO: Epoch 238: valid_loss 3.24 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25.5\n",
            "INFO: Epoch 239: loss 2.268 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.233 | clip 0.8\n",
            "INFO: Epoch 239: valid_loss 3.21 | num_tokens 9.99 | batch_size 500 | valid_perplexity 24.9\n",
            "INFO: Epoch 240: loss 2.256 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.735 | clip 0.7\n",
            "INFO: Epoch 240: valid_loss 3.22 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25\n",
            "INFO: Epoch 241: loss 2.258 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.177 | clip 0.725\n",
            "INFO: Epoch 241: valid_loss 3.21 | num_tokens 9.99 | batch_size 500 | valid_perplexity 24.8\n",
            "INFO: Epoch 242: loss 2.234 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.496 | clip 0.7\n",
            "INFO: Epoch 242: valid_loss 3.22 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25.1\n",
            "INFO: Epoch 243: loss 2.24 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.595 | clip 0.775\n",
            "INFO: Epoch 243: valid_loss 3.21 | num_tokens 9.99 | batch_size 500 | valid_perplexity 24.7\n",
            "INFO: Epoch 244: loss 2.232 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.387 | clip 0.65\n",
            "INFO: Epoch 244: valid_loss 3.22 | num_tokens 9.99 | batch_size 500 | valid_perplexity 25.1\n",
            "INFO: Epoch 245: loss 2.228 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.86 | clip 0.75\n",
            "INFO: Epoch 245: valid_loss 3.2 | num_tokens 9.99 | batch_size 500 | valid_perplexity 24.5\n",
            "INFO: Epoch 246: loss 2.216 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.658 | clip 0.625\n",
            "INFO: Epoch 246: valid_loss 3.21 | num_tokens 9.99 | batch_size 500 | valid_perplexity 24.9\n",
            "INFO: Epoch 247: loss 2.22 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 6.774 | clip 0.75\n",
            "INFO: Epoch 247: valid_loss 3.2 | num_tokens 9.99 | batch_size 500 | valid_perplexity 24.5\n",
            "INFO: Epoch 248: loss 2.209 | lr 0.0003 | num_tokens 11.04 | batch_size 250 | grad_norm 7.253 | clip 0.7\n",
            "INFO: Epoch 248: valid_loss 3.21 | num_tokens 9.99 | batch_size 500 | valid_perplexity 24.9\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python translate.py \\\n",
        "  --data /content/drive/MyDrive/MT/data/en-fr/prepared \\\n",
        "  --dicts /content/drive/MyDrive/MT/data/en-fr/prepared \\\n",
        "  --checkpoint-path /content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_1/checkpoints/checkpoint_best.pt \\\n",
        "  --output /content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_1/translations.txt \\\n",
        "  --cuda \\\n",
        "  --batch-size 1024"
      ],
      "metadata": {
        "id": "MV9D_wUmsKde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5acae6d8-9097-4077-d3bc-e9e5120ef8b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-11-06 12:29:02] COMMAND: translate.py --data /content/drive/MyDrive/MT/data/en-fr/prepared --dicts /content/drive/MyDrive/MT/data/en-fr/prepared --checkpoint-path /content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_1/checkpoints/checkpoint_best.pt --output /content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_1/translations.txt --cuda --batch-size 1024\n",
            "[2022-11-06 12:29:02] Arguments: {'cuda': True, 'data': '/content/drive/MyDrive/MT/data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1024, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/BPE_1/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': '/content/drive/MyDrive/MT/data/en-fr/prepared', 'checkpoint_path': '/content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_1/checkpoints/checkpoint_best.pt', 'output': '/content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_1/translations.txt', 'max_len': 128}\n",
            "[2022-11-06 12:29:02] Loaded a source dictionary (fr) with 4000 words\n",
            "[2022-11-06 12:29:02] Loaded a target dictionary (en) with 4000 words\n",
            "[2022-11-06 12:29:04] Loaded a model from checkpoint /content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_1/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now concatenate the subwords\n",
        "with open(\"/content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_1/translations.txt\") as f:\n",
        "  lines = f.readlines()\n",
        "with open(\"/content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_1/translations.bpe.txt\", 'a') as f:\n",
        "  for line in tqdm(lines):\n",
        "    f.write(decode_line(line))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5PKvzPVr8If",
        "outputId": "0772e509-e27d-46cf-cee0-2198834c8502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 500/500 [00:00<00:00, 123086.75it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! bash /content/drive/MyDrive/MT/atmt_2022/scripts/postprocess.sh \\\n",
        "/content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_1/translations.bpe.txt \\\n",
        "/content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_1/translations.p.txt en"
      ],
      "metadata": {
        "id": "qD0FBEiSstau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1VzLq_0swhw",
        "outputId": "8d8a9a07-c79c-43f3-c05c-4626e78a2398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[K     || 118 kB 26.9 MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2022.6.2)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (4.9.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.21.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.10)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.6.0 sacrebleu-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cat \\\n",
        "  /content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_1/translations.p.txt \\\n",
        "  | sacrebleu /content/drive/MyDrive/MT/data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_kLdXIMs2ck",
        "outputId": "608fd467-a5c3-4460-d1e6-966417c5f922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 9.0,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\",\n",
            " \"verbose_score\": \"33.8/13.4/5.9/2.8 (BP = 0.969 ratio = 0.969 hyp_len = 3772 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.3.1\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BPE Functions and Training\n",
        "## Add on the preprocessing.py maybe?"
      ],
      "metadata": {
        "id": "DSCmdcsXKkKC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjI-iHrBv1PL"
      },
      "outputs": [],
      "source": [
        "import re, collections\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_vocab(filename):\n",
        "    vocab = collections.defaultdict(int)\n",
        "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
        "        for line in fhand:\n",
        "            words = line.strip().split()\n",
        "            for word in words:\n",
        "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "def get_stats(vocab):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "def get_tokens_from_vocab(vocab):\n",
        "    tokens_frequencies = collections.defaultdict(int)\n",
        "    vocab_tokenization = {}\n",
        "    for word, freq in vocab.items():\n",
        "        word_tokens = word.split()\n",
        "        for token in word_tokens:\n",
        "            tokens_frequencies[token] += freq\n",
        "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
        "    return tokens_frequencies, vocab_tokenization\n",
        "\n",
        "def measure_token_length(token):\n",
        "    if token[-4:] == '</w>':\n",
        "        return len(token[:-4]) + 1\n",
        "    else:\n",
        "        return len(token)\n",
        "\n",
        "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
        "    \n",
        "    if string == '':\n",
        "        return []\n",
        "    if sorted_tokens == []:\n",
        "        return [unknown_token]\n",
        "\n",
        "    string_tokens = []\n",
        "    for i in range(len(sorted_tokens)):\n",
        "        token = sorted_tokens[i]\n",
        "        token_reg = re.escape(token.replace('.', '[.]'))\n",
        "\n",
        "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
        "        if len(matched_positions) == 0:\n",
        "            continue\n",
        "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
        "\n",
        "        substring_start_position = 0\n",
        "        for substring_end_position in substring_end_positions:\n",
        "            substring = string[substring_start_position:substring_end_position]\n",
        "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
        "            string_tokens += [token]\n",
        "            substring_start_position = substring_end_position + len(token)\n",
        "        remaining_substring = string[substring_start_position:]\n",
        "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
        "        break\n",
        "    return string_tokens\n",
        "\n",
        "def encode(word):\n",
        "  if word in vocab_tokenization:\n",
        "    return vocab_tokenization[word]\n",
        "  else:\n",
        "    return tokenize_word(string=word, sorted_tokens=sorted_tokens, unknown_token='</u>')\n",
        "\n",
        "def decode_line(line):\n",
        "  words = line.split('</w>')\n",
        "  new_line = ''\n",
        "  for word in words:\n",
        "    new_line = new_line + ' ' + decode_word(word)\n",
        "  return new_line\n",
        "\n",
        "def decode_word(word):\n",
        "  subwords = word.split(' ')\n",
        "  return ''.join(subwords)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# learn BPE segmentation for English\n",
        "vocab = get_vocab('/content/drive/MyDrive/MT/data/en-fr/preprocessed/train.fr')\n",
        "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "\n",
        "num_merges = 5000\n",
        "print(\"## Training BPE model...\")\n",
        "for i in tqdm(range(num_merges)):\n",
        "    pairs = get_stats(vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "\n",
        "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
        "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
        "\n",
        "print('Saving vocab_tokenization and sorted_tokens to $preprocessed...')\n",
        "with open(os.path.join(preprocessed, 'vocab_tokenization_fr_5000.json'), 'w') as f: \n",
        "    json.dump(vocab_tokenization, f)\n",
        "with open(os.path.join(preprocessed, 'sorted_tokens_fr_5000.txt'), \"w\") as f:\n",
        "    f.write(\"\\n\".join(sorted_tokens))\n",
        "\n",
        "# then do the same for french (change the file names to not overwrite English files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkAIPqG8yFcR",
        "outputId": "1da363a3-6c93-4b08-a296-4c9f007e831f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Training BPE model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 5000/5000 [03:52<00:00, 21.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving vocab_tokenization and sorted_tokens to $preprocessed...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now apply BPE on all English files and save to $BPE\n",
        "# first need to add </w to the end of each word\n",
        "# will be useful for separating the words from each other\n",
        "\n",
        "# make new dir, data/en-fr/BPE\n",
        "\n",
        "lang = 'en'\n",
        "for fname in ['train']:\n",
        "  with open(os.path.join(preprocessed, fname + '.' + lang)) as f:\n",
        "    lines = f.readlines()\n",
        "  with open(os.path.join(BPE, fname + '.' + lang), 'a') as f:\n",
        "    for line in tqdm(lines):\n",
        "      words = line.split(' ')\n",
        "      new_words = []\n",
        "      new_line = ''\n",
        "      for word in words:\n",
        "        new_line = new_line + ' ' + ' '.join(encode(word + '</w>'))\n",
        "      f.write(new_line + '\\n')\n",
        "# then do the same for French"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4erGtk2y3SCv",
        "outputId": "489f23a5-41e4-478f-d11e-577449f34a0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 10000/10000 [30:58<00:00,  5.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/MT/data/en-fr/BPE/train.en\") as f:\n",
        "  lines = f.readlines()\n",
        "len(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x13VrxXsuZos",
        "outputId": "ccc21f10-a8df-4be4-fd56-caa97a69fe23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Rest of the Preprocessing"
      ],
      "metadata": {
        "id": "Z0W_2AjvLLpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train truecase model for language in BPE.\n",
        "lang = 'fr'\n",
        "! perl $moses_scripts/train-truecaser.perl --model $BPE/tm.$lang --corpus $BPE/train.$lang"
      ],
      "metadata": {
        "id": "w_XMKVAPLBWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -rf /content/drive/MyDrive/MT/data/en-fr/prepared\n",
        "! mkdir /content/drive/MyDrive/MT/data/en-fr/prepared"
      ],
      "metadata": {
        "id": "UlB4HV3AuqDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python /content/drive/MyDrive/MT/atmt_2022/preprocess.py \\\n",
        "    --source-lang $src_lang \\\n",
        "    --target-lang $tgt_lang \\\n",
        "    --dest-dir $prepared \\\n",
        "    --train-prefix $BPE/train \\\n",
        "    --valid-prefix $BPE/valid \\\n",
        "    --test-prefix $BPE/test \\\n",
        "    --tiny-train-prefix $BPE/tiny_train \\\n",
        "    --threshold-src 1 \\\n",
        "    --threshold-tgt 1 \\\n",
        "    --num-words-src 4000 \\\n",
        "    --num-words-tgt 4000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFYcnj1uvZ-y",
        "outputId": "1b1a42ae-c227-4fc0-b6b9-bb56df106d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-11-06 12:16:51] COMMAND: /content/drive/MyDrive/MT/atmt_2022/preprocess.py --source-lang fr --target-lang en --dest-dir /content/drive/MyDrive/MT/data/en-fr/prepared --train-prefix /content/drive/MyDrive/MT/data/en-fr/BPE/train --valid-prefix /content/drive/MyDrive/MT/data/en-fr/BPE/valid --test-prefix /content/drive/MyDrive/MT/data/en-fr/BPE/test --tiny-train-prefix /content/drive/MyDrive/MT/data/en-fr/BPE/tiny_train --threshold-src 1 --threshold-tgt 1 --num-words-src 4000 --num-words-tgt 4000\n",
            "[2022-11-06 12:16:51] Arguments: {'source_lang': 'fr', 'target_lang': 'en', 'train_prefix': '/content/drive/MyDrive/MT/data/en-fr/BPE/train', 'tiny_train_prefix': '/content/drive/MyDrive/MT/data/en-fr/BPE/tiny_train', 'valid_prefix': '/content/drive/MyDrive/MT/data/en-fr/BPE/valid', 'test_prefix': '/content/drive/MyDrive/MT/data/en-fr/BPE/test', 'dest_dir': '/content/drive/MyDrive/MT/data/en-fr/prepared', 'threshold_src': 1, 'num_words_src': 4000, 'threshold_tgt': 1, 'num_words_tgt': 4000, 'vocab_src': None, 'vocab_trg': None, 'quiet': False}\n",
            "[2022-11-06 12:16:51] COMMAND: /content/drive/MyDrive/MT/atmt_2022/preprocess.py --source-lang fr --target-lang en --dest-dir /content/drive/MyDrive/MT/data/en-fr/prepared --train-prefix /content/drive/MyDrive/MT/data/en-fr/BPE/train --valid-prefix /content/drive/MyDrive/MT/data/en-fr/BPE/valid --test-prefix /content/drive/MyDrive/MT/data/en-fr/BPE/test --tiny-train-prefix /content/drive/MyDrive/MT/data/en-fr/BPE/tiny_train --threshold-src 1 --threshold-tgt 1 --num-words-src 4000 --num-words-tgt 4000\n",
            "[2022-11-06 12:16:51] Arguments: {'source_lang': 'fr', 'target_lang': 'en', 'train_prefix': '/content/drive/MyDrive/MT/data/en-fr/BPE/train', 'tiny_train_prefix': '/content/drive/MyDrive/MT/data/en-fr/BPE/tiny_train', 'valid_prefix': '/content/drive/MyDrive/MT/data/en-fr/BPE/valid', 'test_prefix': '/content/drive/MyDrive/MT/data/en-fr/BPE/test', 'dest_dir': '/content/drive/MyDrive/MT/data/en-fr/prepared', 'threshold_src': 1, 'num_words_src': 4000, 'threshold_tgt': 1, 'num_words_tgt': 4000, 'vocab_src': None, 'vocab_trg': None, 'quiet': False}\n",
            "[2022-11-06 12:16:51] Built a source dictionary (fr) with 4000 words\n",
            "[2022-11-06 12:16:52] Built a target dictionary (en) with 4000 words\n",
            "[2022-11-06 12:16:53] Built a binary dataset for /content/drive/MyDrive/MT/data/en-fr/BPE/train.fr: 10000 sentences, 110434 tokens, 1.061% replaced by unknown token\n",
            "[2022-11-06 12:16:53] Built a binary dataset for /content/drive/MyDrive/MT/data/en-fr/BPE/tiny_train.fr: 1000 sentences, 11308 tokens, 1.380% replaced by unknown token\n",
            "[2022-11-06 12:16:53] Built a binary dataset for /content/drive/MyDrive/MT/data/en-fr/BPE/valid.fr: 500 sentences, 5626 tokens, 1.031% replaced by unknown token\n",
            "[2022-11-06 12:16:53] Built a binary dataset for /content/drive/MyDrive/MT/data/en-fr/BPE/test.fr: 500 sentences, 5661 tokens, 1.307% replaced by unknown token\n",
            "[2022-11-06 12:16:55] Built a binary dataset for /content/drive/MyDrive/MT/data/en-fr/BPE/train.en: 10000 sentences, 97378 tokens, 0.540% replaced by unknown token\n",
            "[2022-11-06 12:16:55] Built a binary dataset for /content/drive/MyDrive/MT/data/en-fr/BPE/tiny_train.en: 1000 sentences, 10003 tokens, 0.770% replaced by unknown token\n",
            "[2022-11-06 12:16:55] Built a binary dataset for /content/drive/MyDrive/MT/data/en-fr/BPE/valid.en: 500 sentences, 4993 tokens, 0.601% replaced by unknown token\n",
            "[2022-11-06 12:16:55] Built a binary dataset for /content/drive/MyDrive/MT/data/en-fr/BPE/test.en: 500 sentences, 5049 tokens, 0.832% replaced by unknown token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BPE Usage"
      ],
      "metadata": {
        "id": "IPlw7U_LKJhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to load the BPE model simply do:\n",
        "with open(os.path.join(preprocessed, 'sorted_tokens_5000.txt'), 'r') as f:\n",
        "  sorted_tokens = f.read().split('\\n')\n",
        "with open(os.path.join(preprocessed, 'vocab_tokenization_5000.json'), 'r') as f:\n",
        "  vocab_tokenization = json.load(f)"
      ],
      "metadata": {
        "id": "6kyLHz8R-zec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "line = 'hello how are you iloveapples'\n",
        "words = line.split(' ')\n",
        "new_words = []\n",
        "new_line = ''\n",
        "for word in words:\n",
        "  new_line = new_line + ' ' + ' '.join(encode(word+'</w>'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgRzWKzC-heK",
        "outputId": "75f57beb-c2d3-4c83-c52b-4ab610b49653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n",
            "how\n",
            "are\n",
            "you\n",
            "illuminati\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iyHbr-MB-nMf",
        "outputId": "ad222e2b-ec11-4ee2-c3b8-4f0d8ccc26b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' hello</w> how</w> are</w> you</w> ill u min ati </w>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode_line(new_line)"
      ],
      "metadata": {
        "id": "Bk_gA6fZHram"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}