{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSaIchQYgSs_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e53e905a-c319-4cbd-d278-74357cd1672b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "%cd /content/drive/MyDrive/MT/atmt_2022"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POHzmCkgRXU5",
        "outputId": "10a66673-eb94-4ba0-9871-32ad3294d821"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MT/atmt_2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "hNKfQc_bLhpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy data folder to somewhere else for testing\n",
        "! cp -r /content/drive/MyDrive/MT/atmt_2022/data /content/drive/MyDrive/MT/data3"
      ],
      "metadata": {
        "id": "Poza-xtjovr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vars to run the preprocess.py script\n",
        "# we start from the already preprocessed data in $preprocessed\n",
        "# we apply the BPE to all the files and save all of them to $BPE\n",
        "data_folder = '/content/drive/MyDrive/MT/data3'\n",
        "\n",
        "base = '/content/drive/MyDrive/MT/atmt_2022'\n",
        "moses_scripts = os.path.join(base, 'scripts/../moses_scripts')\n",
        "postprocess_script = os.path.join(base, 'scripts/postprocess.sh')\n",
        "preprocess_script = os.path.join(base, 'preprocess.py')\n",
        "\n",
        "preprocessed = os.path.join(data_folder, 'en-fr/preprocessed')\n",
        "raw_data = os.path.join(data_folder, 'en-fr/raw')\n",
        "prepared = os.path.join(data_folder, 'en-fr/prepared')\n",
        "BPE = os.path.join(data_folder, 'en-fr/BPE')\n",
        "test_data = os.path.join(data_folder, 'en-fr/raw/test.en')\n",
        "\n",
        "src_lang = 'fr'\n",
        "tgt_lang = 'en'"
      ],
      "metadata": {
        "id": "azzhVspmstNA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# $prepared need to be empty\n",
        "# we have empty BPE and prepared now\n",
        "! rm -rf $prepared\n",
        "! mkdir $prepared\n",
        "! mkdir $BPE"
      ],
      "metadata": {
        "id": "2Gz27VDL1Vb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Evaluate Model\n",
        "See BPE encoding and preprocessing below first."
      ],
      "metadata": {
        "id": "af2wh_EMLxK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_file = os.path.join(base, 'assignments/03/BPE_2')\n",
        "checkpoints = os.path.join(out_file, 'checkpoints')\n",
        "saved_model = os.path.join(checkpoints, 'checkpoint_best.pt')\n",
        "# for versioning"
      ],
      "metadata": {
        "id": "6CYl51QxtjUQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py \\\n",
        "--data $prepared \\\n",
        "--source-lang $src_lang \\\n",
        "--target-lang $tgt_lang \\\n",
        "--save-dir $checkpoints \\\n",
        "--cuda \\\n",
        "--batch-size 256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ADEWWUgRTqP",
        "outputId": "8abd08da-b2bd-4cc8-9afb-f7eb4ceaebd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data /content/drive/MyDrive/MT/data3/en-fr/prepared --source-lang fr --target-lang en --save-dir assignments/03/BPE_2/checkpoints --cuda --batch-size 256\n",
            "INFO: Arguments: {'cuda': True, 'data': '/content/drive/MyDrive/MT/data3/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 256, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/BPE_2/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 7.879 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.661 | clip 0.5\n",
            "INFO: Epoch 000: valid_loss 6.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 680\n",
            "INFO: Epoch 001: loss 5.901 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 11.71 | clip 1\n",
            "INFO: Epoch 001: valid_loss 5.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 214\n",
            "INFO: Epoch 002: loss 5.417 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.431 | clip 0.7\n",
            "INFO: Epoch 002: valid_loss 5.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 182\n",
            "INFO: Epoch 003: loss 5.291 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.874 | clip 0.9\n",
            "INFO: Epoch 003: valid_loss 5.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 169\n",
            "INFO: Epoch 004: loss 5.159 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 8.07 | clip 0.9\n",
            "INFO: Epoch 004: valid_loss 5.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 154\n",
            "INFO: Epoch 005: loss 5.074 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 8.191 | clip 0.85\n",
            "INFO: Epoch 005: valid_loss 4.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 135\n",
            "INFO: Epoch 006: loss 4.993 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.839 | clip 0.725\n",
            "INFO: Epoch 006: valid_loss 4.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 127\n",
            "INFO: Epoch 007: loss 4.924 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.757 | clip 0.65\n",
            "INFO: Epoch 007: valid_loss 4.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 120\n",
            "INFO: Epoch 008: loss 4.873 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.828 | clip 0.725\n",
            "INFO: Epoch 008: valid_loss 4.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 116\n",
            "INFO: Epoch 009: loss 4.836 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 8.1 | clip 0.8\n",
            "INFO: Epoch 009: valid_loss 4.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 110\n",
            "INFO: Epoch 010: loss 4.791 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 8.262 | clip 0.775\n",
            "INFO: Epoch 010: valid_loss 4.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 105\n",
            "INFO: Epoch 011: loss 4.743 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 8.206 | clip 0.725\n",
            "INFO: Epoch 011: valid_loss 4.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 100\n",
            "INFO: Epoch 012: loss 4.697 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.95 | clip 0.75\n",
            "INFO: Epoch 012: valid_loss 4.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 96.4\n",
            "INFO: Epoch 013: loss 4.655 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.723 | clip 0.725\n",
            "INFO: Epoch 013: valid_loss 4.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 92.7\n",
            "INFO: Epoch 014: loss 4.613 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.644 | clip 0.7\n",
            "INFO: Epoch 014: valid_loss 4.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 89.1\n",
            "INFO: Epoch 015: loss 4.574 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.475 | clip 0.725\n",
            "INFO: Epoch 015: valid_loss 4.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 85.3\n",
            "INFO: Epoch 016: loss 4.532 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.47 | clip 0.7\n",
            "INFO: Epoch 016: valid_loss 4.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 82.4\n",
            "INFO: Epoch 017: loss 4.495 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.365 | clip 0.725\n",
            "INFO: Epoch 017: valid_loss 4.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 79.7\n",
            "INFO: Epoch 018: loss 4.459 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.315 | clip 0.7\n",
            "INFO: Epoch 018: valid_loss 4.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 76.6\n",
            "INFO: Epoch 019: loss 4.422 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.142 | clip 0.675\n",
            "INFO: Epoch 019: valid_loss 4.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 74.2\n",
            "INFO: Epoch 020: loss 4.386 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.954 | clip 0.675\n",
            "INFO: Epoch 020: valid_loss 4.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 71.8\n",
            "INFO: Epoch 021: loss 4.352 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.942 | clip 0.675\n",
            "INFO: Epoch 021: valid_loss 4.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 68.7\n",
            "INFO: Epoch 022: loss 4.31 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.412 | clip 0.625\n",
            "INFO: Epoch 022: valid_loss 4.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 66.5\n",
            "INFO: Epoch 023: loss 4.277 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.188 | clip 0.6\n",
            "INFO: Epoch 023: valid_loss 4.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 64.6\n",
            "INFO: Epoch 024: loss 4.243 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.662 | clip 0.55\n",
            "INFO: Epoch 024: valid_loss 4.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 62\n",
            "INFO: Epoch 025: loss 4.202 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.062 | clip 0.55\n",
            "INFO: Epoch 025: valid_loss 4.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61\n",
            "INFO: Epoch 026: loss 4.176 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.337 | clip 0.525\n",
            "INFO: Epoch 026: valid_loss 4.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58.2\n",
            "INFO: Epoch 027: loss 4.138 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.617 | clip 0.425\n",
            "INFO: Epoch 027: valid_loss 4.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 56.9\n",
            "INFO: Epoch 028: loss 4.107 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.113 | clip 0.45\n",
            "INFO: Epoch 028: valid_loss 4.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55.4\n",
            "INFO: Epoch 029: loss 4.077 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.037 | clip 0.475\n",
            "INFO: Epoch 029: valid_loss 3.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.5\n",
            "INFO: Epoch 030: loss 4.047 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.598 | clip 0.45\n",
            "INFO: Epoch 030: valid_loss 3.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.8\n",
            "INFO: Epoch 031: loss 4.023 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.58 | clip 0.475\n",
            "INFO: Epoch 031: valid_loss 3.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.8\n",
            "INFO: Epoch 032: loss 3.987 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.41 | clip 0.425\n",
            "INFO: Epoch 032: valid_loss 3.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.4\n",
            "INFO: Epoch 033: loss 3.958 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.614 | clip 0.475\n",
            "INFO: Epoch 033: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.2\n",
            "INFO: Epoch 034: loss 3.928 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.205 | clip 0.4\n",
            "INFO: Epoch 034: valid_loss 3.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.1\n",
            "INFO: Epoch 035: loss 3.902 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.112 | clip 0.4\n",
            "INFO: Epoch 035: valid_loss 3.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.8\n",
            "INFO: Epoch 036: loss 3.873 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.83 | clip 0.4\n",
            "INFO: Epoch 036: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.8\n",
            "INFO: Epoch 037: loss 3.85 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.922 | clip 0.35\n",
            "INFO: Epoch 037: valid_loss 3.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.1\n",
            "INFO: Epoch 038: loss 3.825 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.841 | clip 0.4\n",
            "INFO: Epoch 038: valid_loss 3.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.3\n",
            "INFO: Epoch 039: loss 3.801 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.783 | clip 0.35\n",
            "INFO: Epoch 039: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.1\n",
            "INFO: Epoch 040: loss 3.771 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.573 | clip 0.3\n",
            "INFO: Epoch 040: valid_loss 3.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.6\n",
            "INFO: Epoch 041: loss 3.752 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.758 | clip 0.3\n",
            "INFO: Epoch 041: valid_loss 3.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.7\n",
            "INFO: Epoch 042: loss 3.724 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.188 | clip 0.275\n",
            "INFO: Epoch 042: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.3\n",
            "INFO: Epoch 043: loss 3.705 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.253 | clip 0.35\n",
            "INFO: Epoch 043: valid_loss 3.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.4\n",
            "INFO: Epoch 044: loss 3.683 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.965 | clip 0.325\n",
            "INFO: Epoch 044: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.9\n",
            "INFO: Epoch 045: loss 3.666 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.799 | clip 0.325\n",
            "INFO: Epoch 045: valid_loss 3.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38\n",
            "INFO: Epoch 046: loss 3.641 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.581 | clip 0.325\n",
            "INFO: Epoch 046: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.2\n",
            "INFO: Epoch 047: loss 3.617 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.671 | clip 0.325\n",
            "INFO: Epoch 047: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.7\n",
            "INFO: Epoch 048: loss 3.595 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.68 | clip 0.3\n",
            "INFO: Epoch 048: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.1\n",
            "INFO: Epoch 049: loss 3.575 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.141 | clip 0.3\n",
            "INFO: Epoch 049: valid_loss 3.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.3\n",
            "INFO: Epoch 050: loss 3.552 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.525 | clip 0.325\n",
            "INFO: Epoch 050: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.9\n",
            "INFO: Epoch 051: loss 3.527 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.717 | clip 0.35\n",
            "INFO: Epoch 051: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.2\n",
            "INFO: Epoch 052: loss 3.512 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.866 | clip 0.325\n",
            "INFO: Epoch 052: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.9\n",
            "INFO: Epoch 053: loss 3.492 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.798 | clip 0.325\n",
            "INFO: Epoch 053: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.8\n",
            "INFO: Epoch 054: loss 3.463 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.776 | clip 0.3\n",
            "INFO: Epoch 054: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.7\n",
            "INFO: Epoch 055: loss 3.449 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.722 | clip 0.375\n",
            "INFO: Epoch 055: valid_loss 3.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.7\n",
            "INFO: Epoch 056: loss 3.426 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.041 | clip 0.375\n",
            "INFO: Epoch 056: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.3\n",
            "INFO: Epoch 057: loss 3.403 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.871 | clip 0.3\n",
            "INFO: Epoch 057: valid_loss 3.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31\n",
            "INFO: Epoch 058: loss 3.396 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.208 | clip 0.425\n",
            "INFO: Epoch 058: valid_loss 3.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.4\n",
            "INFO: Epoch 059: loss 3.371 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.78 | clip 0.35\n",
            "INFO: Epoch 059: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.8\n",
            "INFO: Epoch 060: loss 3.348 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.648 | clip 0.35\n",
            "INFO: Epoch 060: valid_loss 3.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.4\n",
            "INFO: Epoch 061: loss 3.328 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.62 | clip 0.35\n",
            "INFO: Epoch 061: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.8\n",
            "INFO: Epoch 062: loss 3.308 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.858 | clip 0.375\n",
            "INFO: Epoch 062: valid_loss 3.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.5\n",
            "INFO: Epoch 063: loss 3.294 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.763 | clip 0.35\n",
            "INFO: Epoch 063: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.1\n",
            "INFO: Epoch 064: loss 3.275 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.24 | clip 0.4\n",
            "INFO: Epoch 064: valid_loss 3.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.7\n",
            "INFO: Epoch 065: loss 3.258 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.899 | clip 0.4\n",
            "INFO: Epoch 065: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.4\n",
            "INFO: Epoch 066: loss 3.246 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.139 | clip 0.4\n",
            "INFO: Epoch 066: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.1\n",
            "INFO: Epoch 067: loss 3.227 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.044 | clip 0.4\n",
            "INFO: Epoch 067: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.6\n",
            "INFO: Epoch 068: loss 3.212 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.717 | clip 0.375\n",
            "INFO: Epoch 068: valid_loss 3.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.8\n",
            "INFO: Epoch 069: loss 3.206 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.051 | clip 0.4\n",
            "INFO: Epoch 069: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26\n",
            "INFO: Epoch 070: loss 3.183 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.012 | clip 0.325\n",
            "INFO: Epoch 070: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.8\n",
            "INFO: Epoch 071: loss 3.17 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.89 | clip 0.375\n",
            "INFO: Epoch 071: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.4\n",
            "INFO: Epoch 072: loss 3.151 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.071 | clip 0.325\n",
            "INFO: Epoch 072: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.3\n",
            "INFO: Epoch 073: loss 3.141 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.81 | clip 0.425\n",
            "INFO: Epoch 073: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.9\n",
            "INFO: Epoch 074: loss 3.121 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.84 | clip 0.375\n",
            "INFO: Epoch 074: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.6\n",
            "INFO: Epoch 075: loss 3.111 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.896 | clip 0.375\n",
            "INFO: Epoch 075: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.4\n",
            "INFO: Epoch 076: loss 3.099 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.219 | clip 0.4\n",
            "INFO: Epoch 076: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.1\n",
            "INFO: Epoch 077: loss 3.083 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.893 | clip 0.425\n",
            "INFO: Epoch 077: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 078: loss 3.074 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.958 | clip 0.375\n",
            "INFO: Epoch 078: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.7\n",
            "INFO: Epoch 079: loss 3.056 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.965 | clip 0.425\n",
            "INFO: Epoch 079: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.5\n",
            "INFO: Epoch 080: loss 3.048 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.918 | clip 0.425\n",
            "INFO: Epoch 080: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23\n",
            "INFO: Epoch 081: loss 3.029 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.089 | clip 0.375\n",
            "INFO: Epoch 081: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23\n",
            "INFO: Epoch 082: loss 3.022 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 4.943 | clip 0.425\n",
            "INFO: Epoch 082: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.6\n",
            "INFO: Epoch 083: loss 3.006 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.878 | clip 0.375\n",
            "INFO: Epoch 083: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.8\n",
            "INFO: Epoch 084: loss 2.995 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.456 | clip 0.45\n",
            "INFO: Epoch 084: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.1\n",
            "INFO: Epoch 085: loss 2.98 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.866 | clip 0.4\n",
            "INFO: Epoch 085: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.2\n",
            "INFO: Epoch 086: loss 2.971 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.4 | clip 0.45\n",
            "INFO: Epoch 086: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.8\n",
            "INFO: Epoch 087: loss 2.958 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.553 | clip 0.4\n",
            "INFO: Epoch 087: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 088: loss 2.951 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.219 | clip 0.475\n",
            "INFO: Epoch 088: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.4\n",
            "INFO: Epoch 089: loss 2.93 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.5 | clip 0.425\n",
            "INFO: Epoch 089: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.4\n",
            "INFO: Epoch 090: loss 2.925 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.407 | clip 0.5\n",
            "INFO: Epoch 090: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21\n",
            "INFO: Epoch 091: loss 2.912 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.095 | clip 0.4\n",
            "INFO: Epoch 091: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21\n",
            "INFO: Epoch 092: loss 2.899 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.423 | clip 0.425\n",
            "INFO: Epoch 092: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.7\n",
            "INFO: Epoch 093: loss 2.882 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.357 | clip 0.475\n",
            "INFO: Epoch 093: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.7\n",
            "INFO: Epoch 094: loss 2.88 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.296 | clip 0.45\n",
            "INFO: Epoch 094: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.3\n",
            "INFO: Epoch 095: loss 2.858 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.448 | clip 0.425\n",
            "INFO: Epoch 095: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2\n",
            "INFO: Epoch 096: loss 2.855 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.415 | clip 0.475\n",
            "INFO: Epoch 096: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20\n",
            "INFO: Epoch 097: loss 2.839 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.099 | clip 0.425\n",
            "INFO: Epoch 097: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2\n",
            "INFO: Epoch 098: loss 2.84 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.633 | clip 0.55\n",
            "INFO: Epoch 098: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.7\n",
            "INFO: Epoch 099: loss 2.818 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.791 | clip 0.45\n",
            "INFO: Epoch 099: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.6\n",
            "INFO: Epoch 100: loss 2.813 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.566 | clip 0.475\n",
            "INFO: Epoch 100: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.5\n",
            "INFO: Epoch 101: loss 2.796 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.374 | clip 0.45\n",
            "INFO: Epoch 101: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.6\n",
            "INFO: Epoch 102: loss 2.787 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.515 | clip 0.5\n",
            "INFO: Epoch 102: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 103: loss 2.777 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.715 | clip 0.45\n",
            "INFO: Epoch 103: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19\n",
            "INFO: Epoch 104: loss 2.763 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.399 | clip 0.55\n",
            "INFO: Epoch 104: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.8\n",
            "INFO: Epoch 105: loss 2.748 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.592 | clip 0.475\n",
            "INFO: Epoch 105: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.9\n",
            "INFO: Epoch 106: loss 2.748 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.928 | clip 0.5\n",
            "INFO: Epoch 106: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.5\n",
            "INFO: Epoch 107: loss 2.735 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.66 | clip 0.4\n",
            "INFO: Epoch 107: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.7\n",
            "INFO: Epoch 108: loss 2.728 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.577 | clip 0.525\n",
            "INFO: Epoch 108: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.3\n",
            "INFO: Epoch 109: loss 2.71 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.081 | clip 0.5\n",
            "INFO: Epoch 109: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.4\n",
            "INFO: Epoch 110: loss 2.709 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.441 | clip 0.575\n",
            "INFO: Epoch 110: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18\n",
            "INFO: Epoch 111: loss 2.691 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.602 | clip 0.475\n",
            "INFO: Epoch 111: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.2\n",
            "INFO: Epoch 112: loss 2.688 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.639 | clip 0.525\n",
            "INFO: Epoch 112: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.8\n",
            "INFO: Epoch 113: loss 2.672 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.991 | clip 0.5\n",
            "INFO: Epoch 113: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.8\n",
            "INFO: Epoch 114: loss 2.67 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.652 | clip 0.45\n",
            "INFO: Epoch 114: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.5\n",
            "INFO: Epoch 115: loss 2.653 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.422 | clip 0.45\n",
            "INFO: Epoch 115: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6\n",
            "INFO: Epoch 116: loss 2.646 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.56 | clip 0.45\n",
            "INFO: Epoch 116: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.4\n",
            "INFO: Epoch 117: loss 2.632 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.439 | clip 0.475\n",
            "INFO: Epoch 117: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.3\n",
            "INFO: Epoch 118: loss 2.624 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.67 | clip 0.525\n",
            "INFO: Epoch 118: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.2\n",
            "INFO: Epoch 119: loss 2.615 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.429 | clip 0.525\n",
            "INFO: Epoch 119: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17\n",
            "INFO: Epoch 120: loss 2.606 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.697 | clip 0.5\n",
            "INFO: Epoch 120: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.1\n",
            "INFO: Epoch 121: loss 2.601 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.827 | clip 0.525\n",
            "INFO: Epoch 121: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 122: loss 2.59 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.761 | clip 0.475\n",
            "INFO: Epoch 122: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.9\n",
            "INFO: Epoch 123: loss 2.578 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.805 | clip 0.525\n",
            "INFO: Epoch 123: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.6\n",
            "INFO: Epoch 124: loss 2.573 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.024 | clip 0.525\n",
            "INFO: Epoch 124: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 125: loss 2.566 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.998 | clip 0.55\n",
            "INFO: Epoch 125: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 126: loss 2.549 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.163 | clip 0.475\n",
            "INFO: Epoch 126: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7\n",
            "INFO: Epoch 127: loss 2.55 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.386 | clip 0.65\n",
            "INFO: Epoch 127: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2\n",
            "INFO: Epoch 128: loss 2.532 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.004 | clip 0.55\n",
            "INFO: Epoch 128: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 129: loss 2.523 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.201 | clip 0.525\n",
            "INFO: Epoch 129: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 130: loss 2.517 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.127 | clip 0.5\n",
            "INFO: Epoch 130: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2\n",
            "INFO: Epoch 131: loss 2.513 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.69 | clip 0.55\n",
            "INFO: Epoch 131: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 132: loss 2.5 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.17 | clip 0.525\n",
            "INFO: Epoch 132: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 133: loss 2.493 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.985 | clip 0.575\n",
            "INFO: Epoch 133: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 134: loss 2.479 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.145 | clip 0.55\n",
            "INFO: Epoch 134: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 135: loss 2.472 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.013 | clip 0.575\n",
            "INFO: Epoch 135: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 136: loss 2.463 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.485 | clip 0.55\n",
            "INFO: Epoch 136: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 137: loss 2.458 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.942 | clip 0.625\n",
            "INFO: Epoch 137: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 138: loss 2.444 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.501 | clip 0.575\n",
            "INFO: Epoch 138: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 139: loss 2.442 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.956 | clip 0.6\n",
            "INFO: Epoch 139: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 140: loss 2.426 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.432 | clip 0.575\n",
            "INFO: Epoch 140: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 141: loss 2.428 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.082 | clip 0.6\n",
            "INFO: Epoch 141: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 142: loss 2.416 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.167 | clip 0.575\n",
            "INFO: Epoch 142: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 143: loss 2.414 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.649 | clip 0.55\n",
            "INFO: Epoch 143: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.9\n",
            "INFO: Epoch 144: loss 2.391 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.114 | clip 0.575\n",
            "INFO: Epoch 144: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 145: loss 2.389 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.465 | clip 0.65\n",
            "INFO: Epoch 145: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 146: loss 2.381 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.756 | clip 0.575\n",
            "INFO: Epoch 146: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 147: loss 2.38 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.065 | clip 0.65\n",
            "INFO: Epoch 147: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 148: loss 2.363 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.103 | clip 0.6\n",
            "INFO: Epoch 148: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 149: loss 2.363 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.647 | clip 0.575\n",
            "INFO: Epoch 149: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 150: loss 2.351 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.409 | clip 0.65\n",
            "INFO: Epoch 150: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 151: loss 2.343 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.188 | clip 0.6\n",
            "INFO: Epoch 151: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 152: loss 2.339 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.124 | clip 0.625\n",
            "INFO: Epoch 152: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 153: loss 2.327 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.283 | clip 0.625\n",
            "INFO: Epoch 153: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 154: loss 2.319 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.225 | clip 0.675\n",
            "INFO: Epoch 154: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 155: loss 2.309 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 5.974 | clip 0.6\n",
            "INFO: Epoch 155: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 156: loss 2.302 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.814 | clip 0.6\n",
            "INFO: Epoch 156: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 157: loss 2.297 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.32 | clip 0.625\n",
            "INFO: Epoch 157: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 158: loss 2.285 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.214 | clip 0.6\n",
            "INFO: Epoch 158: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 159: loss 2.282 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.571 | clip 0.625\n",
            "INFO: Epoch 159: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 160: loss 2.277 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.637 | clip 0.625\n",
            "INFO: Epoch 160: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 161: loss 2.264 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.578 | clip 0.6\n",
            "INFO: Epoch 161: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 162: loss 2.266 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.479 | clip 0.65\n",
            "INFO: Epoch 162: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 163: loss 2.249 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.681 | clip 0.625\n",
            "INFO: Epoch 163: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 164: loss 2.249 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.492 | clip 0.625\n",
            "INFO: Epoch 164: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 165: loss 2.233 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.193 | clip 0.6\n",
            "INFO: Epoch 165: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 166: loss 2.231 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.369 | clip 0.675\n",
            "INFO: Epoch 166: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 167: loss 2.221 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.418 | clip 0.65\n",
            "INFO: Epoch 167: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 168: loss 2.221 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.917 | clip 0.65\n",
            "INFO: Epoch 168: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 169: loss 2.211 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 8.227 | clip 0.65\n",
            "INFO: Epoch 169: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 170: loss 2.209 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.871 | clip 0.65\n",
            "INFO: Epoch 170: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 171: loss 2.198 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.288 | clip 0.65\n",
            "INFO: Epoch 171: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 172: loss 2.19 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 6.751 | clip 0.675\n",
            "INFO: Epoch 172: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 173: loss 2.183 | lr 0.0003 | num_tokens 10.13 | batch_size 250 | grad_norm 7.021 | clip 0.625\n",
            "INFO: Epoch 173: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_translations = os.path.join(out_file, 'translations.txt')\n",
        "bpe_processed_translations = os.path.join(out_file, \"translations.bpe.txt\")\n",
        "final_translations = os.path.join(out_file, \"translations.p.txt\")"
      ],
      "metadata": {
        "id": "a7aTvM7OaSws"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python translate.py \\\n",
        "  --data $prepared \\\n",
        "  --dicts $prepared \\\n",
        "  --checkpoint-path $saved_model \\\n",
        "  --output $raw_translations \\\n",
        "  --cuda \\\n",
        "  --batch-size 1024"
      ],
      "metadata": {
        "id": "MV9D_wUmsKde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42fc08fd-bc1a-4753-ba12-c612c96e6fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-11-07 16:11:07] COMMAND: translate.py --data /content/drive/MyDrive/MT/data3/en-fr/prepared --dicts /content/drive/MyDrive/MT/data3/en-fr/prepared --checkpoint-path /content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_2/checkpoints/checkpoint_best.pt --output /content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_2/translations.txt --cuda --batch-size 1024\n",
            "[2022-11-07 16:11:07] Arguments: {'cuda': True, 'data': '/content/drive/MyDrive/MT/data3/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1024, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/BPE_2/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': '/content/drive/MyDrive/MT/data3/en-fr/prepared', 'checkpoint_path': '/content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_2/checkpoints/checkpoint_best.pt', 'output': '/content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_2/translations.txt', 'max_len': 128}\n",
            "[2022-11-07 16:11:07] Loaded a source dictionary (fr) with 4000 words\n",
            "[2022-11-07 16:11:07] Loaded a target dictionary (en) with 4000 words\n",
            "[2022-11-07 16:11:10] Loaded a model from checkpoint /content/drive/MyDrive/MT/atmt_2022/assignments/03/BPE_2/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now concatenate the subwords\n",
        "with open(raw_translations) as f:\n",
        "  lines = f.readlines()\n",
        "with open(bpe_processed_translations, 'a') as f:\n",
        "  for line in tqdm(lines):\n",
        "    # decode line in BPE functions part\n",
        "    f.write(decode_line(line))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5PKvzPVr8If",
        "outputId": "bc1961b3-1d23-4865-b08a-768fda4327b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 500/500 [00:00<00:00, 120581.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! bash $postprocess_script $bpe_processed_translations $final_translations en"
      ],
      "metadata": {
        "id": "qD0FBEiSstau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1VzLq_0swhw",
        "outputId": "3288b5f7-ed9c-4206-902f-e94580be293e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[K     || 118 kB 14.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2022.6.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.21.6)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (4.9.1)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.6.0 sacrebleu-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cat $final_translations | sacrebleu $test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_kLdXIMs2ck",
        "outputId": "90eeb210-a9d9-4791-e6ca-e207914eb91e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 11.0,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\",\n",
            " \"verbose_score\": \"44.5/15.6/7.0/3.1 (BP = 1.000 ratio = 1.047 hyp_len = 4073 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.3.1\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BPE Functions and Training\n",
        "Add on the preprocessing.py maybe?"
      ],
      "metadata": {
        "id": "DSCmdcsXKkKC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fjI-iHrBv1PL"
      },
      "outputs": [],
      "source": [
        "import re, collections\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_vocab(filenames):\n",
        "    vocab = collections.defaultdict(int)\n",
        "    for filename in filenames:\n",
        "      with open(filename, 'r', encoding='utf-8') as fhand:\n",
        "          for line in fhand:\n",
        "              words = line.strip().split()\n",
        "              for word in words:\n",
        "                  vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "def get_stats(vocab):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "def get_tokens_from_vocab(vocab):\n",
        "    tokens_frequencies = collections.defaultdict(int)\n",
        "    vocab_tokenization = {}\n",
        "    for word, freq in vocab.items():\n",
        "        word_tokens = word.split()\n",
        "        for token in word_tokens:\n",
        "            tokens_frequencies[token] += freq\n",
        "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
        "    return tokens_frequencies, vocab_tokenization\n",
        "\n",
        "def measure_token_length(token):\n",
        "    if token[-4:] == '</w>':\n",
        "        return len(token[:-4]) + 1\n",
        "    else:\n",
        "        return len(token)\n",
        "\n",
        "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
        "    \n",
        "    if string == '':\n",
        "        return []\n",
        "    if sorted_tokens == []:\n",
        "        return [unknown_token]\n",
        "\n",
        "    string_tokens = []\n",
        "    for i in range(len(sorted_tokens)):\n",
        "        token = sorted_tokens[i]\n",
        "        token_reg = re.escape(token.replace('.', '[.]'))\n",
        "\n",
        "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
        "        if len(matched_positions) == 0:\n",
        "            continue\n",
        "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
        "\n",
        "        substring_start_position = 0\n",
        "        for substring_end_position in substring_end_positions:\n",
        "            substring = string[substring_start_position:substring_end_position]\n",
        "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
        "            string_tokens += [token]\n",
        "            substring_start_position = substring_end_position + len(token)\n",
        "        remaining_substring = string[substring_start_position:]\n",
        "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
        "        break\n",
        "    return string_tokens\n",
        "\n",
        "def encode(word):\n",
        "  if word in vocab_tokenization:\n",
        "    return vocab_tokenization[word]\n",
        "  else:\n",
        "    return tokenize_word(string=word, sorted_tokens=sorted_tokens, unknown_token='</u>')\n",
        "\n",
        "def decode_line(line):\n",
        "  words = line.split('</w>')\n",
        "  new_line = ''\n",
        "  for word in words:\n",
        "    new_line = new_line + ' ' + decode_word(word)\n",
        "  return new_line\n",
        "\n",
        "def decode_word(word):\n",
        "  subwords = word.split(' ')\n",
        "  return ''.join(subwords)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# learn BPE segmentation for both languages\n",
        "vocab = get_vocab([os.path.join(preprocessed, 'train.en'), os.path.join(preprocessed, 'train.fr')])\n",
        "\n",
        "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "\n",
        "num_merges = 10000\n",
        "print(\"## Training BPE model...\")\n",
        "for i in tqdm(range(num_merges)):\n",
        "    pairs = get_stats(vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "\n",
        "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
        "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
        "\n",
        "print('Saving vocab_tokenization and sorted_tokens to $preprocessed...')\n",
        "with open(os.path.join(preprocessed, 'vocab_tokenization.json'), 'w') as f: \n",
        "    json.dump(vocab_tokenization, f)\n",
        "with open(os.path.join(preprocessed, 'sorted_tokens.txt'), \"w\") as f:\n",
        "    f.write(\"\\n\".join(sorted_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkAIPqG8yFcR",
        "outputId": "c19c1fdc-0b78-436f-87c3-483437395435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Training BPE model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 10000/10000 [15:59<00:00, 10.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving vocab_tokenization and sorted_tokens to $preprocessed...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now apply BPE on all files and save to $BPE\n",
        "# first need to add </w> to the end of each word\n",
        "# will be useful for separating the words from each other\n",
        "langs = ['fr', 'en']\n",
        "for lang in langs:\n",
        "  for fname in ['train', 'test', 'valid', 'tiny_train']:\n",
        "    with open(os.path.join(preprocessed, fname + '.' + lang)) as f:\n",
        "      lines = f.readlines()\n",
        "    with open(os.path.join(BPE, fname + '.' + lang), 'a') as f:\n",
        "      for line in tqdm(lines):\n",
        "        words = line[:-1].split(' ')\n",
        "        new_words = []\n",
        "        new_line = ''\n",
        "        for word in words:\n",
        "          new_line = new_line + ' ' + ''.join(encode(word + '</w>'))\n",
        "        f.write(new_line + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4erGtk2y3SCv",
        "outputId": "a970beb0-a16c-46cb-e551-4c1ef886d64b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 10000/10000 [00:00<00:00, 88855.78it/s]\n",
            "100%|| 500/500 [01:30<00:00,  5.52it/s]\n",
            "100%|| 500/500 [01:25<00:00,  5.82it/s]\n",
            "100%|| 1000/1000 [02:48<00:00,  5.95it/s]\n",
            "100%|| 10000/10000 [00:00<00:00, 128758.37it/s]\n",
            "100%|| 500/500 [01:07<00:00,  7.44it/s]\n",
            "100%|| 500/500 [00:55<00:00,  9.09it/s]\n",
            "100%|| 1000/1000 [02:01<00:00,  8.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Rest of the Preprocessing"
      ],
      "metadata": {
        "id": "Z0W_2AjvLLpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train truecase model for languages in BPE.\n",
        "! perl $moses_scripts/train-truecaser.perl --model $BPE/tm.$lang --corpus $BPE/train.$src_lang\n",
        "! perl $moses_scripts/train-truecaser.perl --model $BPE/tm.$lang --corpus $BPE/train.$tgt_lang"
      ],
      "metadata": {
        "id": "w_XMKVAPLBWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python $preprocess_script \\\n",
        "    --source-lang $src_lang \\\n",
        "    --target-lang $tgt_lang \\\n",
        "    --dest-dir $prepared \\\n",
        "    --train-prefix $BPE/train \\\n",
        "    --valid-prefix $BPE/valid \\\n",
        "    --test-prefix $BPE/test \\\n",
        "    --tiny-train-prefix $BPE/tiny_train \\\n",
        "    --threshold-src 1 \\\n",
        "    --threshold-tgt 1 \\\n",
        "    --num-words-src 4000 \\\n",
        "    --num-words-tgt 4000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFYcnj1uvZ-y",
        "outputId": "8d4b0ab6-fb59-4012-ea68-10542b2933ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-11-07 16:03:23] COMMAND: /content/drive/MyDrive/MT/atmt_2022/preprocess.py --source-lang fr --target-lang en --dest-dir /content/drive/MyDrive/MT/data3/en-fr/prepared --train-prefix /content/drive/MyDrive/MT/data3/en-fr/BPE/train --valid-prefix /content/drive/MyDrive/MT/data3/en-fr/BPE/valid --test-prefix /content/drive/MyDrive/MT/data3/en-fr/BPE/test --tiny-train-prefix /content/drive/MyDrive/MT/data3/en-fr/BPE/tiny_train --threshold-src 1 --threshold-tgt 1 --num-words-src 4000 --num-words-tgt 4000\n",
            "[2022-11-07 16:03:23] Arguments: {'source_lang': 'fr', 'target_lang': 'en', 'train_prefix': '/content/drive/MyDrive/MT/data3/en-fr/BPE/train', 'tiny_train_prefix': '/content/drive/MyDrive/MT/data3/en-fr/BPE/tiny_train', 'valid_prefix': '/content/drive/MyDrive/MT/data3/en-fr/BPE/valid', 'test_prefix': '/content/drive/MyDrive/MT/data3/en-fr/BPE/test', 'dest_dir': '/content/drive/MyDrive/MT/data3/en-fr/prepared', 'threshold_src': 1, 'num_words_src': 4000, 'threshold_tgt': 1, 'num_words_tgt': 4000, 'vocab_src': None, 'vocab_trg': None, 'quiet': False}\n",
            "[2022-11-07 16:03:23] COMMAND: /content/drive/MyDrive/MT/atmt_2022/preprocess.py --source-lang fr --target-lang en --dest-dir /content/drive/MyDrive/MT/data3/en-fr/prepared --train-prefix /content/drive/MyDrive/MT/data3/en-fr/BPE/train --valid-prefix /content/drive/MyDrive/MT/data3/en-fr/BPE/valid --test-prefix /content/drive/MyDrive/MT/data3/en-fr/BPE/test --tiny-train-prefix /content/drive/MyDrive/MT/data3/en-fr/BPE/tiny_train --threshold-src 1 --threshold-tgt 1 --num-words-src 4000 --num-words-tgt 4000\n",
            "[2022-11-07 16:03:23] Arguments: {'source_lang': 'fr', 'target_lang': 'en', 'train_prefix': '/content/drive/MyDrive/MT/data3/en-fr/BPE/train', 'tiny_train_prefix': '/content/drive/MyDrive/MT/data3/en-fr/BPE/tiny_train', 'valid_prefix': '/content/drive/MyDrive/MT/data3/en-fr/BPE/valid', 'test_prefix': '/content/drive/MyDrive/MT/data3/en-fr/BPE/test', 'dest_dir': '/content/drive/MyDrive/MT/data3/en-fr/prepared', 'threshold_src': 1, 'num_words_src': 4000, 'threshold_tgt': 1, 'num_words_tgt': 4000, 'vocab_src': None, 'vocab_trg': None, 'quiet': False}\n",
            "[2022-11-07 16:03:25] Built a source dictionary (fr) with 4000 words\n",
            "[2022-11-07 16:03:25] Built a target dictionary (en) with 4000 words\n",
            "[2022-11-07 16:03:27] Built a binary dataset for /content/drive/MyDrive/MT/data3/en-fr/BPE/train.fr: 10000 sentences, 100137 tokens, 4.242% replaced by unknown token\n",
            "[2022-11-07 16:03:27] Built a binary dataset for /content/drive/MyDrive/MT/data3/en-fr/BPE/tiny_train.fr: 1000 sentences, 10029 tokens, 6.621% replaced by unknown token\n",
            "[2022-11-07 16:03:28] Built a binary dataset for /content/drive/MyDrive/MT/data3/en-fr/BPE/valid.fr: 500 sentences, 5037 tokens, 6.154% replaced by unknown token\n",
            "[2022-11-07 16:03:28] Built a binary dataset for /content/drive/MyDrive/MT/data3/en-fr/BPE/test.fr: 500 sentences, 4990 tokens, 6.774% replaced by unknown token\n",
            "[2022-11-07 16:03:30] Built a binary dataset for /content/drive/MyDrive/MT/data3/en-fr/BPE/train.en: 10000 sentences, 91000 tokens, 2.431% replaced by unknown token\n",
            "[2022-11-07 16:03:30] Built a binary dataset for /content/drive/MyDrive/MT/data3/en-fr/BPE/tiny_train.en: 1000 sentences, 9142 tokens, 4.605% replaced by unknown token\n",
            "[2022-11-07 16:03:31] Built a binary dataset for /content/drive/MyDrive/MT/data3/en-fr/BPE/valid.en: 500 sentences, 4568 tokens, 4.313% replaced by unknown token\n",
            "[2022-11-07 16:03:31] Built a binary dataset for /content/drive/MyDrive/MT/data3/en-fr/BPE/test.en: 500 sentences, 4560 tokens, 5.197% replaced by unknown token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BPE Usage"
      ],
      "metadata": {
        "id": "IPlw7U_LKJhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to load the BPE model simply do:\n",
        "with open(os.path.join(preprocessed, 'sorted_tokens.txt'), 'r') as f:\n",
        "  sorted_tokens = f.read().split('\\n')\n",
        "with open(os.path.join(preprocessed, 'vocab_tokenization.json'), 'r') as f:\n",
        "  vocab_tokenization = json.load(f)"
      ],
      "metadata": {
        "id": "6kyLHz8R-zec"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "line = 'hello how are you iloveapples'\n",
        "words = line.split(' ')\n",
        "new_words = []\n",
        "new_line = ''\n",
        "for word in words:\n",
        "  new_line = new_line + ' ' + ' '.join(encode(word+'</w>'))"
      ],
      "metadata": {
        "id": "xgRzWKzC-heK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iyHbr-MB-nMf",
        "outputId": "817f6eec-09bb-4c8c-843b-501f486e2609"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' hello</w> how</w> are</w> you</w> ilo ve apples</w>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode_line(new_line)"
      ],
      "metadata": {
        "id": "Bk_gA6fZHram",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "7fb78742-bdd6-41c6-c979-119621bb39de"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' hello how are you iloveapples '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}